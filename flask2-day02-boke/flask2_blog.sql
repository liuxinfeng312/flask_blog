-- MySQL dump 10.13  Distrib 5.7.25, for Linux (x86_64)
--
-- Host: localhost    Database: flask2_boke
-- ------------------------------------------------------
-- Server version	5.7.25-0ubuntu0.16.04.2

/*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */;
/*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */;
/*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */;
/*!40101 SET NAMES utf8 */;
/*!40103 SET @OLD_TIME_ZONE=@@TIME_ZONE */;
/*!40103 SET TIME_ZONE='+00:00' */;
/*!40014 SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0 */;
/*!40014 SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0 */;
/*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE='NO_AUTO_VALUE_ON_ZERO' */;
/*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */;

--
-- Table structure for table `alembic_version`
--

DROP TABLE IF EXISTS `alembic_version`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `alembic_version` (
  `version_num` varchar(32) NOT NULL,
  PRIMARY KEY (`version_num`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `alembic_version`
--

LOCK TABLES `alembic_version` WRITE;
/*!40000 ALTER TABLE `alembic_version` DISABLE KEYS */;
INSERT INTO `alembic_version` VALUES ('571ef3461082');
/*!40000 ALTER TABLE `alembic_version` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `article`
--

DROP TABLE IF EXISTS `article`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `article` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `title` varchar(100) DEFAULT NULL,
  `context` text,
  `time` date DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=12 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `article`
--

LOCK TABLES `article` WRITE;
/*!40000 ALTER TABLE `article` DISABLE KEYS */;
INSERT INTO `article` VALUES (5,'动态网页数据抓取','<p># 动态网页数据抓取</p><p><br/></p><p>## 什么是AJAX：</p><p><br/></p><p>AJAX（Asynchronouse JavaScript And XML）异步JavaScript和XML。过在后台与服务器进行少量数据交换，Ajax 可以使网页实现异步更新。这意味着可以在不重新加载整个网页的情况下，对网页的某部分进行更新。传统的网页（不使用Ajax）如果需要更新内容，必须重载整个网页页面。因为传统的在传输数据格式方面，使用的是`XML`语法。因此叫做`AJAX`，其实现在数据交互基本上都是使用`JSON`。使用AJAX加载的数据，即使使用了JS，将数据渲染到了浏览器中，在`右键-&gt;查看网页源代码`还是不能看到通过ajax加载的数据，只能看到使用这个url加载的html代码。</p><p><br/></p><p>## 获取ajax数据的方式：</p><p><br/></p><p>1. 直接分析ajax调用的接口。然后通过代码请求这个接口。</p><p>2. 使用Selenium+chromedriver模拟浏览器行为获取数据。</p><p><br/></p><p>| 方式&nbsp; &nbsp; &nbsp;| 优点&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| 缺点&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;|</p><p>| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |</p><p>| 分析接口 | 直接可以请求到数据。不需要做一些解析工作。代码量少，性能高。 | 分析接口比较复杂，特别是一些通过js混淆的接口，要有一定的js功底。容易被发现是爬虫。 |</p><p>| selenium | 直接模拟浏览器的行为。浏览器能请求到的，使用selenium也能请求到。爬虫更稳定。 | 代码量多。性能低。&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;|</p><p><br/></p><p>## Selenium+chromedriver获取动态数据：</p><p><br/></p><p>`Selenium`相当于是一个机器人。可以模拟人类在浏览器上的一些行为，自动处理浏览器上的一些行为，比如点击，填充数据，删除cookie等。`chromedriver`是一个驱动`Chrome`浏览器的驱动程序，使用他才可以驱动浏览器。当然针对不同的浏览器有不同的driver。以下列出了不同浏览器及其对应的driver：</p><p><br/></p><p>1. Chrome：&lt;https://sites.google.com/a/chromium.org/chromedriver/downloads&gt;</p><p>2. Firefox：&lt;https://github.com/mozilla/geckodriver/releases&gt;</p><p>3. Edge：&lt;https://developer.microsoft.com/en-us/microsoft-edge/tools/webdriver/&gt;</p><p>4. Safari：&lt;https://webkit.org/blog/6900/webdriver-support-in-safari-10/&gt;</p><p><br/></p><p>## 安装Selenium和chromedriver：</p><p><br/></p><p>1. 安装Selenium：</p><p><br/></p><p>&nbsp; &nbsp;`Selenium`有很多语言的版本，有java、ruby、python等。我们下载python版本的就可以了。</p><p><br/></p><p>&nbsp; &nbsp;```</p><p>&nbsp; &nbsp; pip install selenium</p><p>&nbsp; &nbsp;```</p><p><br/></p><p>2. 安装`chromedriver`：下载完成后，放到不需要权限的纯英文目录下就可以了。</p><p><br/></p><p>### 快速入门：</p><p><br/></p><p>现在以一个简单的获取百度首页的例子来讲下`Selenium`和`chromedriver`如何快速入门：</p><p><br/></p><p>```</p><p>from selenium import webdriver</p><p><br/></p><p># chromedriver的绝对路径</p><p>driver_path = r&#39;D:\\ProgramApp\\chromedriver\\chromedriver.exe&#39;</p><p><br/></p><p># 初始化一个driver，并且指定chromedriver的路径</p><p>driver = webdriver.Chrome(executable_path=driver_path)</p><p># 请求网页</p><p>driver.get(&quot;https://www.baidu.com/&quot;)</p><p># 通过page_source获取网页源代码</p><p>print(driver.page_source)</p><p>```</p><p><br/></p><p>### selenium常用操作：</p><p><br/></p><p>更多教程请参考：&lt;http://selenium-python.readthedocs.io/installation.html#introduction&gt;</p><p><br/></p><p>#### 关闭页面：</p><p><br/></p><p>1. `driver.close()`：关闭当前页面。</p><p>2. `driver.quit()`：退出整个浏览器。</p><p><br/></p><p>#### 定位元素：</p><p><br/></p><p>1. `find_element_by_id`：根据id来查找某个元素。等价于：</p><p><br/></p><p>&nbsp; &nbsp;```</p><p>&nbsp; &nbsp; submitTag = driver.find_element_by_id(&#39;su&#39;)</p><p>&nbsp; &nbsp; submitTag1 = driver.find_element(By.ID,&#39;su&#39;)</p><p>&nbsp; &nbsp;```</p><p><br/></p><p>2. `find_element_by_class_name`：根据类名查找元素。 等价于：</p><p><br/></p><p>&nbsp; &nbsp;```</p><p>&nbsp; &nbsp; submitTag = driver.find_element_by_class_name(&#39;su&#39;)</p><p>&nbsp; &nbsp; submitTag1 = driver.find_element(By.CLASS_NAME,&#39;su&#39;)</p><p>&nbsp; &nbsp;```</p><p><br/></p><p>3. `find_element_by_name`：根据name属性的值来查找元素。等价于：</p><p><br/></p><p>&nbsp; &nbsp;```</p><p>&nbsp; &nbsp; submitTag = driver.find_element_by_name(&#39;email&#39;)</p><p>&nbsp; &nbsp; submitTag1 = driver.find_element(By.NAME,&#39;email&#39;)</p><p>&nbsp; &nbsp;```</p><p><br/></p><p>4. `find_element_by_tag_name`：根据标签名来查找元素。等价于：</p><p><br/></p><p>&nbsp; &nbsp;```</p><p>&nbsp; &nbsp; submitTag = driver.find_element_by_tag_name(&#39;div&#39;)</p><p>&nbsp; &nbsp; submitTag1 = driver.find_element(By.TAG_NAME,&#39;div&#39;)</p><p>&nbsp; &nbsp;```</p><p><br/></p><p>5. `find_element_by_xpath`：根据xpath语法来获取元素。等价于：</p><p><br/></p><p>&nbsp; &nbsp;```</p><p>&nbsp; &nbsp; submitTag = driver.find_element_by_xpath(&#39;//div&#39;)</p><p>&nbsp; &nbsp; submitTag1 = driver.find_element(By.XPATH,&#39;//div&#39;)</p><p>&nbsp; &nbsp;```</p><p><br/></p><p>6. `find_element_by_css_selector`：根据css选择器选择元素。等价于：</p><p><br/></p><p>&nbsp; &nbsp;```</p><p>&nbsp; &nbsp; submitTag = driver.find_element_by_css_selector(&#39;//div&#39;)</p><p>&nbsp; &nbsp; submitTag1 = driver.find_element(By.CSS_SELECTOR,&#39;//div&#39;)</p><p>&nbsp; &nbsp;```</p><p><br/></p><p>&nbsp; &nbsp;要注意，`find_element`是获取第一个满足条件的元素。`find_elements`是获取所有满足条件的元素。</p><p><br/></p><p>#### 操作表单元素：</p><p><br/></p><p>1. 操作输入框：分为两步。第一步：找到这个元素。第二步：使用`send_keys(value)`，将数据填充进去。示例代码如下：</p><p><br/></p><p>&nbsp; &nbsp;```</p><p>&nbsp; &nbsp; inputTag = driver.find_element_by_id(&#39;kw&#39;)</p><p>&nbsp; &nbsp; inputTag.send_keys(&#39;python&#39;)</p><p>&nbsp; &nbsp;```</p><p><br/></p><p>&nbsp; &nbsp;使用`clear`方法可以清除输入框中的内容。示例代码如下：</p><p><br/></p><p>&nbsp; &nbsp;```</p><p>&nbsp; &nbsp; inputTag.clear()</p><p>&nbsp; &nbsp;```</p><p><br/></p><p>2. 操作checkbox：因为要选中`checkbox`标签，在网页中是通过鼠标点击的。因此想要选中`checkbox`标签，那么先选中这个标签，然后执行`click`事件。示例代码如下：</p><p><br/></p><p>&nbsp; &nbsp;```</p><p>&nbsp; &nbsp; rememberTag = driver.find_element_by_name(&quot;rememberMe&quot;)</p><p>&nbsp; &nbsp; rememberTag.click()</p><p>&nbsp; &nbsp;```</p><p><br/></p><p>3. 选择select：select元素不能直接点击。因为点击后还需要选中元素。这时候selenium就专门为select标签提供了一个类`selenium.webdriver.support.ui.Select`。将获取到的元素当成参数传到这个类中，创建这个对象。以后就可以使用这个对象进行选择了。示例代码如下：</p><p><br/></p><p>&nbsp; &nbsp;```</p><p>&nbsp; &nbsp; from selenium.webdriver.support.ui import Select</p><p>&nbsp; &nbsp; # 选中这个标签，然后使用Select创建对象</p><p>&nbsp; &nbsp; selectTag = Select(driver.find_element_by_name(&quot;jumpMenu&quot;))</p><p>&nbsp; &nbsp; # 根据索引选择</p><p>&nbsp; &nbsp; selectTag.select_by_index(1)</p><p>&nbsp; &nbsp; # 根据值选择</p><p>&nbsp; &nbsp; selectTag.select_by_value(&quot;http://www.95yueba.com&quot;)</p><p>&nbsp; &nbsp; # 根据可视的文本选择</p><p>&nbsp; &nbsp; selectTag.select_by_visible_text(&quot;95秀客户端&quot;)</p><p>&nbsp; &nbsp; # 取消选中所有选项</p><p>&nbsp; &nbsp; selectTag.deselect_all()</p><p>&nbsp; &nbsp;```</p><p><br/></p><p>4. 操作按钮：操作按钮有很多种方式。比如单击、右击、双击等。这里讲一个最常用的。就是点击。直接调用`click`函数就可以了。示例代码如下：</p><p><br/></p><p>&nbsp; &nbsp;```</p><p>&nbsp; &nbsp; inputTag = driver.find_element_by_id(&#39;su&#39;)</p><p>&nbsp; &nbsp; inputTag.click()</p><p>&nbsp; &nbsp;```</p><p><br/></p><p>#### 行为链：</p><p><br/></p><p>有时候在页面中的操作可能要有很多步，那么这时候可以使用鼠标行为链类`ActionChains`来完成。比如现在要将鼠标移动到某个元素上并执行点击事件。那么示例代码如下：</p><p><br/></p><p>```</p><p>inputTag = driver.find_element_by_id(&#39;kw&#39;)</p><p>submitTag = driver.find_element_by_id(&#39;su&#39;)</p><p><br/></p><p>actions = ActionChains(driver)</p><p>actions.move_to_element(inputTag)</p><p>actions.send_keys_to_element(inputTag,&#39;python&#39;)</p><p>actions.move_to_element(submitTag)</p><p>actions.click(submitTag)</p><p>actions.perform()</p><p>```</p><p><br/></p><p>还有更多的鼠标相关的操作。</p><p><br/></p><p>- click_and_hold(element)：点击但不松开鼠标。</p><p>- context_click(element)：右键点击。</p><p>- double_click(element)：双击。 更多方法请参考：&lt;https://selenium-python.readthedocs.io/api.html#module-selenium.webdriver.common.action_chains&gt;</p><p><br/></p><p>#### Cookie操作：</p><p><br/></p><p>1. 获取所有的cookie：</p><p><br/></p><p>&nbsp; &nbsp;```</p><p>&nbsp; &nbsp; for cookie in driver.get_cookies():</p><p>&nbsp; &nbsp; &nbsp; &nbsp; print(cookie)</p><p>&nbsp; &nbsp;```</p><p><br/></p><p>2. 根据cookie的key获取value：</p><p><br/></p><p>&nbsp; &nbsp;```</p><p>&nbsp; &nbsp; value = driver.get_cookie(key)</p><p>&nbsp; &nbsp;```</p><p><br/></p><p>3. 删除所有的cookie：</p><p><br/></p><p>&nbsp; &nbsp;```</p><p>&nbsp; &nbsp; driver.delete_all_cookies()</p><p>&nbsp; &nbsp;```</p><p><br/></p><p>4. 删除某个cookie：</p><p><br/></p><p>&nbsp; &nbsp;```</p><p>&nbsp; &nbsp; driver.delete_cookie(key)</p><p>&nbsp; &nbsp;```</p><p><br/></p><p>#### 页面等待：</p><p><br/></p><p>现在的网页越来越多采用了 Ajax 技术，这样程序便不能确定何时某个元素完全加载出来了。如果实际页面等待时间过长导致某个dom元素还没出来，但是你的代码直接使用了这个WebElement，那么就会抛出NullPointer的异常。为了解决这个问题。所以 Selenium 提供了两种等待方式：一种是隐式等待、一种是显式等待。</p><p><br/></p><p>1. 隐式等待：调用`driver.implicitly_wait`。那么在获取不可用的元素之前，会先等待10秒中的时间。示例代码如下：</p><p><br/></p><p>&nbsp; &nbsp;```</p><p>&nbsp; &nbsp;driver = webdriver.Chrome(executable_path=driver_path)</p><p>&nbsp; &nbsp;driver.implicitly_wait(10)</p><p>&nbsp; &nbsp;# 请求网页</p><p>&nbsp; &nbsp;driver.get(&quot;https://www.douban.com/&quot;)</p><p>&nbsp; &nbsp;```</p><p><br/></p><p>2. 显示等待：显示等待是表明某个条件成立后才执行获取元素的操作。也可以在等待的时候指定一个最大的时间，如果超过这个时间那么就抛出一个异常。显示等待应该使用`from selenium.webdriver.support import expected_conditions as EC`期望的条件和`selenium.webdriver.support.ui.WebDriverWait`来配合完成。示例代码如下：</p><p><br/></p><p>&nbsp; &nbsp;```</p><p>&nbsp; &nbsp; from selenium import webdriver</p><p>&nbsp; &nbsp; from selenium.webdriver.common.by import By</p><p>&nbsp; &nbsp; from selenium.webdriver.support.ui import WebDriverWait</p><p>&nbsp; &nbsp; from selenium.webdriver.support import expected_conditions as EC</p><p>&nbsp; &nbsp;</p><p>&nbsp; &nbsp; driver = webdriver.Firefox()</p><p>&nbsp; &nbsp; driver.get(&quot;http://somedomain/url_that_delays_loading&quot;)</p><p>&nbsp; &nbsp; try:</p><p>&nbsp; &nbsp; &nbsp; &nbsp; element = WebDriverWait(driver, 10).until(</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; EC.presence_of_element_located((By.ID, &quot;myDynamicElement&quot;))</p><p>&nbsp; &nbsp; &nbsp; &nbsp; )</p><p>&nbsp; &nbsp; finally:</p><p>&nbsp; &nbsp; &nbsp; &nbsp; driver.quit()</p><p>&nbsp; &nbsp;```</p><p><br/></p><p>3. 一些其他的等待条件：</p><p><br/></p><p>&nbsp; &nbsp;- presence_of_element_located：某个元素已经加载完毕了。</p><p><br/></p><p>&nbsp; &nbsp;- presence_of_all_emement_located：网页中所有满足条件的元素都加载完毕了。</p><p><br/></p><p>&nbsp; &nbsp;- element_to_be_cliable：某个元素是可以点击了。</p><p><br/></p><p>&nbsp; &nbsp; &nbsp;更多条件请参考：&lt;http://selenium-python.readthedocs.io/waits.html&gt;</p><p><br/></p><p>#### 切换页面：</p><p><br/></p><p>有时候窗口中有很多子tab页面。这时候肯定是需要进行切换的。`selenium`提供了一个叫做`switch_to_window`来进行切换，具体切换到哪个页面，可以从`driver.window_handles`中找到。示例代码如下：</p><p><br/></p><p>```</p><p># 打开一个新的页面</p><p>self.driver.execute_script(&quot;window.open(&#39;&quot;+url+&quot;&#39;)&quot;)</p><p># 切换到这个新的页面中</p><p>self.driver.switch_to_window(self.driver.window_handles[1])</p><p>```</p><p><br/></p><p>#### 设置代理ip：</p><p><br/></p><p>有时候频繁爬取一些网页。服务器发现你是爬虫后会封掉你的ip地址。这时候我们可以更改代理ip。更改代理ip，不同的浏览器有不同的实现方式。这里以`Chrome`浏览器为例来讲解：</p><p><br/></p><p>```</p><p>from selenium import webdriver</p><p><br/></p><p>options = webdriver.ChromeOptions()</p><p>options.add_argument(&quot;--proxy-server=http://110.73.2.248:8123&quot;)</p><p>driver_path = r&quot;D:\\ProgramApp\\chromedriver\\chromedriver.exe&quot;</p><p>driver = webdriver.Chrome(executable_path=driver_path,chrome_options=options)</p><p><br/></p><p>driver.get(&#39;http://httpbin.org/ip&#39;)</p><p>```</p><p><br/></p><p>#### `WebElement`元素：</p><p><br/></p><p>`from selenium.webdriver.remote.webelement import WebElement`类是每个获取出来的元素的所属类。</p><p>有一些常用的属性：</p><p><br/></p><p>1. get_attribute：这个标签的某个属性的值。</p><p>2. screentshot：获取当前页面的截图。这个方法只能在`driver`上使用。</p><p>&nbsp; &nbsp;`driver`的对象类，也是继承自`WebElement`。</p><p>&nbsp; &nbsp;更多请阅读相关源代码。</p><p><br/></p>','2019-04-20'),(6,'多线程爬虫','<p># 多线程爬虫</p><p><br/></p><p>有些时候，比如下载图片，因为下载图片是一个耗时的操作。如果采用之前那种同步的方式下载。那效率肯会特别慢。这时候我们就可以考虑使用多线程的方式来下载图片。</p><p><br/></p><p>## 多线程介绍：</p><p><br/></p><p>多线程是为了同步完成多项任务，通过提高资源使用效率来提高系统的效率。线程是在同一时间需要完成多项任务的时候实现的。</p><p>最简单的比喻多线程就像火车的每一节车厢，而进程则是火车。车厢离开火车是无法跑动的，同理火车也可以有多节车厢。多线程的出现就是为了提高效率。同时它的出现也带来了一些问题。更多介绍请参考：&lt;https://baike.baidu.com/item/多线程/1190404?fr=aladdin&gt;</p><p><br/></p><p>## threading模块介绍：</p><p><br/></p><p>`threading`模块是`python`中专门提供用来做多线程编程的模块。`threading`模块中最常用的类是`Thread`。以下看一个简单的多线程程序：</p><p><br/></p><p>```</p><p>import threading</p><p>import time</p><p><br/></p><p>def coding():</p><p>&nbsp; &nbsp; for x in range(3):</p><p>&nbsp; &nbsp; &nbsp; &nbsp; print(&#39;%s正在写代码&#39; % x)</p><p>&nbsp; &nbsp; &nbsp; &nbsp; time.sleep(1)</p><p><br/></p><p>def drawing():</p><p>&nbsp; &nbsp; for x in range(3):</p><p>&nbsp; &nbsp; &nbsp; &nbsp; print(&#39;%s正在画图&#39; % x)</p><p>&nbsp; &nbsp; &nbsp; &nbsp; time.sleep(1)</p><p><br/></p><p><br/></p><p>def single_thread():</p><p>&nbsp; &nbsp; coding()</p><p>&nbsp; &nbsp; drawing()</p><p><br/></p><p>def multi_thread():</p><p>&nbsp; &nbsp; t1 = threading.Thread(target=coding)</p><p>&nbsp; &nbsp; t2 = threading.Thread(target=drawing)</p><p><br/></p><p>&nbsp; &nbsp; t1.start()</p><p>&nbsp; &nbsp; t2.start()</p><p><br/></p><p>if __name__ == &#39;__main__&#39;:</p><p>&nbsp; &nbsp; multi_thread()</p><p>```</p><p><br/></p><p>### 查看线程数：</p><p><br/></p><p>使用`threading.enumerate()`函数便可以看到当前线程的数量。</p><p><br/></p><p>### 查看当前线程的名字：</p><p><br/></p><p>使用`threading.current_thread()`可以看到当前线程的信息。</p><p><br/></p><p>### 继承自`threading.Thread`类：</p><p><br/></p><p>为了让线程代码更好的封装。可以使用`threading`模块下的`Thread`类，继承自这个类，然后实现`run`方法，线程就会自动运行`run`方法中的代码。示例代码如下：</p><p><br/></p><p>```</p><p>import threading</p><p>import time</p><p><br/></p><p>class CodingThread(threading.Thread):</p><p>&nbsp; &nbsp; def run(self):</p><p>&nbsp; &nbsp; &nbsp; &nbsp; for x in range(3):</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; print(&#39;%s正在写代码&#39; % threading.current_thread())</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; time.sleep(1)</p><p><br/></p><p>class DrawingThread(threading.Thread):</p><p>&nbsp; &nbsp; def run(self):</p><p>&nbsp; &nbsp; &nbsp; &nbsp; for x in range(3):</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; print(&#39;%s正在画图&#39; % threading.current_thread())</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; time.sleep(1)</p><p><br/></p><p>def multi_thread():</p><p>&nbsp; &nbsp; t1 = CodingThread()</p><p>&nbsp; &nbsp; t2 = DrawingThread()</p><p><br/></p><p>&nbsp; &nbsp; t1.start()</p><p>&nbsp; &nbsp; t2.start()</p><p><br/></p><p>if __name__ == &#39;__main__&#39;:</p><p>&nbsp; &nbsp; multi_thread()</p><p>```</p><p><br/></p><p>### 多线程共享全局变量的问题：</p><p><br/></p><p>多线程都是在同一个进程中运行的。因此在进程中的全局变量所有线程都是可共享的。这就造成了一个问题，因为线程执行的顺序是无序的。有可能会造成数据错误。比如以下代码：</p><p><br/></p><p>```</p><p>import threading</p><p><br/></p><p>tickets = 0</p><p><br/></p><p>def get_ticket():</p><p>&nbsp; &nbsp; global tickets</p><p>&nbsp; &nbsp; for x in range(1000000):</p><p>&nbsp; &nbsp; &nbsp; &nbsp; tickets += 1</p><p>&nbsp; &nbsp; print(&#39;tickets:%d&#39;%tickets)</p><p><br/></p><p>def main():</p><p>&nbsp; &nbsp; for x in range(2):</p><p>&nbsp; &nbsp; &nbsp; &nbsp; t = threading.Thread(target=get_ticket)</p><p>&nbsp; &nbsp; &nbsp; &nbsp; t.start()</p><p><br/></p><p>if __name__ == &#39;__main__&#39;:</p><p>&nbsp; &nbsp; main()</p><p>```</p><p><br/></p><p>以上结果正常来讲应该是6，但是因为多线程运行的不确定性。因此最后的结果可能是随机的。</p><p><br/></p><p>### 锁机制：</p><p><br/></p><p>为了解决以上使用共享全局变量的问题。`threading`提供了一个`Lock`类，这个类可以在某个线程访问某个变量的时候加锁，其他线程此时就不能进来，直到当前线程处理完后，把锁释放了，其他线程才能进来处理。示例代码如下：</p><p><br/></p><p>```</p><p>import threading</p><p><br/></p><p>VALUE = 0</p><p><br/></p><p>gLock = threading.Lock()</p><p><br/></p><p>def add_value():</p><p>&nbsp; &nbsp; global VALUE</p><p>&nbsp; &nbsp; gLock.acquire()</p><p>&nbsp; &nbsp; for x in range(1000000):</p><p>&nbsp; &nbsp; &nbsp; &nbsp; VALUE += 1</p><p>&nbsp; &nbsp; gLock.release()</p><p>&nbsp; &nbsp; print(&#39;value：%d&#39;%VALUE)</p><p><br/></p><p>def main():</p><p>&nbsp; &nbsp; for x in range(2):</p><p>&nbsp; &nbsp; &nbsp; &nbsp; t = threading.Thread(target=add_value)</p><p>&nbsp; &nbsp; &nbsp; &nbsp; t.start()</p><p><br/></p><p>if __name__ == &#39;__main__&#39;:</p><p>&nbsp; &nbsp; main()</p><p>```</p><p><br/></p><p>## Lock版本生产者和消费者模式：</p><p><br/></p><p>生产者和消费者模式是多线程开发中经常见到的一种模式。生产者的线程专门用来生产一些数据，然后存放到一个中间的变量中。消费者再从这个中间的变量中取出数据进行消费。但是因为要使用中间变量，中间变量经常是一些全局变量，因此需要使用锁来保证数据完整性。以下是使用`threading.Lock`锁实现的“生产者与消费者模式”的一个例子：</p><p><br/></p><p>```</p><p>import threading</p><p>import random</p><p>import time</p><p><br/></p><p>gMoney = 1000</p><p>gLock = threading.Lock()</p><p># 记录生产者生产的次数，达到10次就不再生产</p><p>gTimes = 0</p><p><br/></p><p>class Producer(threading.Thread):</p><p>&nbsp; &nbsp; def run(self):</p><p>&nbsp; &nbsp; &nbsp; &nbsp; global gMoney</p><p>&nbsp; &nbsp; &nbsp; &nbsp; global gLock</p><p>&nbsp; &nbsp; &nbsp; &nbsp; global gTimes</p><p>&nbsp; &nbsp; &nbsp; &nbsp; while True:</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; money = random.randint(100, 1000)</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; gLock.acquire()</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; # 如果已经达到10次了，就不再生产了</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if gTimes &gt;= 10:</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; gLock.release()</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; break</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; gMoney += money</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; print(&#39;%s当前存入%s元钱，剩余%s元钱&#39; % (threading.current_thread(), money, gMoney))</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; gTimes += 1</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; time.sleep(0.5)</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; gLock.release()</p><p><br/></p><p>class Consumer(threading.Thread):</p><p>&nbsp; &nbsp; def run(self):</p><p>&nbsp; &nbsp; &nbsp; &nbsp; global gMoney</p><p>&nbsp; &nbsp; &nbsp; &nbsp; global gLock</p><p>&nbsp; &nbsp; &nbsp; &nbsp; global gTimes</p><p>&nbsp; &nbsp; &nbsp; &nbsp; while True:</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; money = random.randint(100, 500)</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; gLock.acquire()</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if gMoney &gt; money:</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; gMoney -= money</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; print(&#39;%s当前取出%s元钱，剩余%s元钱&#39; % (threading.current_thread(), money, gMoney))</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; time.sleep(0.5)</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; else:</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; # 如果钱不够了，有可能是已经超过了次数，这时候就判断一下</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if gTimes &gt;= 10:</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; gLock.release()</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; break</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; print(&quot;%s当前想取%s元钱，剩余%s元钱，不足！&quot; % (threading.current_thread(),money,gMoney))</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; gLock.release()</p><p><br/></p><p>def main():</p><p>&nbsp; &nbsp; for x in range(5):</p><p>&nbsp; &nbsp; &nbsp; &nbsp; Consumer(name=&#39;消费者线程%d&#39;%x).start()</p><p><br/></p><p>&nbsp; &nbsp; for x in range(5):</p><p>&nbsp; &nbsp; &nbsp; &nbsp; Producer(name=&#39;生产者线程%d&#39;%x).start()</p><p><br/></p><p>if __name__ == &#39;__main__&#39;:</p><p>&nbsp; &nbsp; main()</p><p>```</p><p><br/></p><p>## Condition版的生产者与消费者模式：</p><p><br/></p><p>`Lock`版本的生产者与消费者模式可以正常的运行。但是存在一个不足，在消费者中，总是通过`while True`死循环并且上锁的方式去判断钱够不够。上锁是一个很耗费CPU资源的行为。因此这种方式不是最好的。还有一种更好的方式便是使用`threading.Condition`来实现。`threading.Condition`可以在没有数据的时候处于阻塞等待状态。一旦有合适的数据了，还可以使用`notify`相关的函数来通知其他处于等待状态的线程。这样就可以不用做一些无用的上锁和解锁的操作。可以提高程序的性能。首先对`threading.Condition`相关的函数做个介绍，`threading.Condition`类似`threading.Lock`，可以在修改全局数据的时候进行上锁，也可以在修改完毕后进行解锁。以下将一些常用的函数做个简单的介绍：</p><p><br/></p><p>1. `acquire`：上锁。</p><p>2. `release`：解锁。</p><p>3. `wait`：将当前线程处于等待状态，并且会释放锁。可以被其他线程使用`notify`和`notify_all`函数唤醒。被唤醒后会继续等待上锁，上锁后继续执行下面的代码。</p><p>4. `notify`：通知某个正在等待的线程，默认是第1个等待的线程。</p><p>5. `notify_all`：通知所有正在等待的线程。`notify`和`notify_all`不会释放锁。并且需要在`release`之前调用。</p><p><br/></p><p>`Condition`版的生产者与消费者模式代码如下：</p><p><br/></p><p>```</p><p>import threading</p><p>import random</p><p>import time</p><p><br/></p><p>gMoney = 1000</p><p>gCondition = threading.Condition()</p><p>gTimes = 0</p><p>gTotalTimes = 5</p><p><br/></p><p>class Producer(threading.Thread):</p><p>&nbsp; &nbsp; def run(self):</p><p>&nbsp; &nbsp; &nbsp; &nbsp; global gMoney</p><p>&nbsp; &nbsp; &nbsp; &nbsp; global gCondition</p><p>&nbsp; &nbsp; &nbsp; &nbsp; global gTimes</p><p>&nbsp; &nbsp; &nbsp; &nbsp; while True:</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; money = random.randint(100, 1000)</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; gCondition.acquire()</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if gTimes &gt;= gTotalTimes:</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; gCondition.release()</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; print(&#39;当前生产者总共生产了%s次&#39;%gTimes)</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; break</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; gMoney += money</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; print(&#39;%s当前存入%s元钱，剩余%s元钱&#39; % (threading.current_thread(), money, gMoney))</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; gTimes += 1</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; time.sleep(0.5)</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; gCondition.notify_all()</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; gCondition.release()</p><p><br/></p><p>class Consumer(threading.Thread):</p><p>&nbsp; &nbsp; def run(self):</p><p>&nbsp; &nbsp; &nbsp; &nbsp; global gMoney</p><p>&nbsp; &nbsp; &nbsp; &nbsp; global gCondition</p><p>&nbsp; &nbsp; &nbsp; &nbsp; while True:</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; money = random.randint(100, 500)</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; gCondition.acquire()</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; # 这里要给个while循环判断，因为等轮到这个线程的时候</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; # 条件有可能又不满足了</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; while gMoney &lt; money:</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if gTimes &gt;= gTotalTimes:</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; gCondition.release()</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; return</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; print(&#39;%s准备取%s元钱，剩余%s元钱，不足！&#39;%(threading.current_thread(),money,gMoney))</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; gCondition.wait()</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; gMoney -= money</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; print(&#39;%s当前取出%s元钱，剩余%s元钱&#39; % (threading.current_thread(), money, gMoney))</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; time.sleep(0.5)</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; gCondition.release()</p><p><br/></p><p>def main():</p><p>&nbsp; &nbsp; for x in range(5):</p><p>&nbsp; &nbsp; &nbsp; &nbsp; Consumer(name=&#39;消费者线程%d&#39;%x).start()</p><p><br/></p><p>&nbsp; &nbsp; for x in range(2):</p><p>&nbsp; &nbsp; &nbsp; &nbsp; Producer(name=&#39;生产者线程%d&#39;%x).start()</p><p><br/></p><p>if __name__ == &#39;__main__&#39;:</p><p>&nbsp; &nbsp; main()</p><p>```</p><p><br/></p><p>## Queue线程安全队列：</p><p><br/></p><p>在线程中，访问一些全局变量，加锁是一个经常的过程。如果你是想把一些数据存储到某个队列中，那么Python内置了一个线程安全的模块叫做`queue`模块。Python中的queue模块中提供了同步的、线程安全的队列类，包括FIFO（先进先出）队列Queue，LIFO（后入先出）队列LifoQueue。这些队列都实现了锁原语（可以理解为原子操作，即要么不做，要么都做完），能够在多线程中直接使用。可以使用队列来实现线程间的同步。相关的函数如下：</p><p><br/></p><p>1. 初始化Queue(maxsize)：创建一个先进先出的队列。</p><p>2. qsize()：返回队列的大小。</p><p>3. empty()：判断队列是否为空。</p><p>4. full()：判断队列是否满了。</p><p>5. get()：从队列中取最后一个数据。</p><p>6. put()：将一个数据放到队列中。</p><p><br/></p><p>## 使用生产者与消费者模式多线程下载表情包：</p><p><br/></p><p>```</p><p>import threading</p><p>import requests</p><p>from lxml import etree</p><p>from urllib import request</p><p>import os</p><p>import re</p><p>from queue import Queue</p><p><br/></p><p>class Producer(threading.Thread):</p><p>&nbsp; &nbsp; headers = {</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36&#39;</p><p>&nbsp; &nbsp; }</p><p>&nbsp; &nbsp; def __init__(self,page_queue,img_queue,*args,**kwargs):</p><p>&nbsp; &nbsp; &nbsp; &nbsp; super(Producer, self).__init__(*args,**kwargs)</p><p>&nbsp; &nbsp; &nbsp; &nbsp; self.page_queue = page_queue</p><p>&nbsp; &nbsp; &nbsp; &nbsp; self.img_queue = img_queue</p><p><br/></p><p><br/></p><p>&nbsp; &nbsp; def run(self):</p><p>&nbsp; &nbsp; &nbsp; &nbsp; while True:</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if self.page_queue.empty():</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; break</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; url = self.page_queue.get()</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; self.parse_page(url)</p><p><br/></p><p>&nbsp; &nbsp; def parse_page(self,url):</p><p>&nbsp; &nbsp; &nbsp; &nbsp; response = requests.get(url,headers=self.headers)</p><p>&nbsp; &nbsp; &nbsp; &nbsp; text = response.text</p><p>&nbsp; &nbsp; &nbsp; &nbsp; html = etree.HTML(text)</p><p>&nbsp; &nbsp; &nbsp; &nbsp; imgs = html.xpath(&quot;//div[@class=&#39;page-content text-center&#39;]//a//img&quot;)</p><p>&nbsp; &nbsp; &nbsp; &nbsp; for img in imgs:</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if img.get(&#39;class&#39;) == &#39;gif&#39;:</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; continue</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; img_url = img.xpath(&quot;.//@data-original&quot;)[0]</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; suffix = os.path.splitext(img_url)[1]</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; alt = img.xpath(&quot;.//@alt&quot;)[0]</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; alt = re.sub(r&#39;[，。？?,/\\\\·]&#39;,&#39;&#39;,alt)</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; img_name = alt + suffix</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; self.img_queue.put((img_url,img_name))</p><p><br/></p><p>class Consumer(threading.Thread):</p><p>&nbsp; &nbsp; def __init__(self,page_queue,img_queue,*args,**kwargs):</p><p>&nbsp; &nbsp; &nbsp; &nbsp; super(Consumer, self).__init__(*args,**kwargs)</p><p>&nbsp; &nbsp; &nbsp; &nbsp; self.page_queue = page_queue</p><p>&nbsp; &nbsp; &nbsp; &nbsp; self.img_queue = img_queue</p><p><br/></p><p>&nbsp; &nbsp; def run(self):</p><p>&nbsp; &nbsp; &nbsp; &nbsp; while True:</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if self.img_queue.empty():</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if self.page_queue.empty():</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; return</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; img = self.img_queue.get(block=True)</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; url,filename = img</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; request.urlretrieve(url,&#39;images/&#39;+filename)</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; print(filename+&#39;&nbsp; 下载完成！&#39;)</p><p><br/></p><p>def main():</p><p>&nbsp; &nbsp; page_queue = Queue(100)</p><p>&nbsp; &nbsp; img_queue = Queue(500)</p><p>&nbsp; &nbsp; for x in range(1,101):</p><p>&nbsp; &nbsp; &nbsp; &nbsp; url = &quot;http://www.doutula.com/photo/list/?page=%d&quot; % x</p><p>&nbsp; &nbsp; &nbsp; &nbsp; page_queue.put(url)</p><p><br/></p><p>&nbsp; &nbsp; for x in range(5):</p><p>&nbsp; &nbsp; &nbsp; &nbsp; t = Producer(page_queue,img_queue)</p><p>&nbsp; &nbsp; &nbsp; &nbsp; t.start()</p><p><br/></p><p>&nbsp; &nbsp; for x in range(5):</p><p>&nbsp; &nbsp; &nbsp; &nbsp; t = Consumer(page_queue,img_queue)</p><p>&nbsp; &nbsp; &nbsp; &nbsp; t.start()</p><p><br/></p><p>if __name__ == &#39;__main__&#39;:</p><p>&nbsp; &nbsp; main()</p><p>```</p><p><br/></p><p>## GIL全局解释器锁：</p><p><br/></p><p>&gt; python 设计之初就是为了线程的安全&nbsp; 同一时刻只能由一个线程运行&nbsp; &nbsp;</p><p><br/></p><p>Python自带的解释器是`CPython`。`CPython`解释器的多线程实际上是一个假的多线程（在多核CPU中，只能利用一核，不能利用多核）。同一时刻只有一个线程在执行，为了保证同一时刻只有一个线程在执行，在`CPython`解释器中有一个东西叫做`GIL（Global Intepreter Lock）`，叫做全局解释器锁。这个解释器锁是有必要的。因为`CPython`解释器的内存管理不是线程安全的。当然除了`CPython`解释器，还有其他的解释器，有些解释器是没有`GIL`锁的，见下面：</p><p><br/></p><p>1. `Jython`：用Java实现的Python解释器。不存在GIL锁。更多详情请见：&lt;https://zh.wikipedia.org/wiki/Jython&gt;</p><p>2. `IronPython`：用`.net`实现的Python解释器。不存在GIL锁。更多详情请见：&lt;https://zh.wikipedia.org/wiki/IronPython&gt;</p><p>3. `PyPy`：用`Python`实现的Python解释器。存在GIL锁。更多详情请见：&lt;https://zh.wikipedia.org/wiki/PyPy&gt;</p><p>&nbsp; &nbsp;GIL虽然是一个假的多线程。但是在处理一些IO操作（比如文件读写和网络请求）还是可以在很大程度上提高效率的。在IO操作上建议使用多线程提高效率。在一些CPU计算操作上不建议使用多线程，而建议使用多进程。</p><p><br/></p><p>## 多线程下载百思不得姐段子作业：</p><p><br/></p><p>```</p><p>import requests</p><p>from lxml import etree</p><p>import threading</p><p>from queue import Queue</p><p>import csv</p><p><br/></p><p><br/></p><p>class BSSpider(threading.Thread):</p><p>&nbsp; &nbsp; headers = {</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36&#39;</p><p>&nbsp; &nbsp; }</p><p>&nbsp; &nbsp; def __init__(self,page_queue,joke_queue,*args,**kwargs):</p><p>&nbsp; &nbsp; &nbsp; &nbsp; super(BSSpider, self).__init__(*args,**kwargs)</p><p>&nbsp; &nbsp; &nbsp; &nbsp; self.base_domain = &#39;http://www.budejie.com&#39;</p><p>&nbsp; &nbsp; &nbsp; &nbsp; self.page_queue = page_queue</p><p>&nbsp; &nbsp; &nbsp; &nbsp; self.joke_queue = joke_queue</p><p><br/></p><p>&nbsp; &nbsp; def run(self):</p><p>&nbsp; &nbsp; &nbsp; &nbsp; while True:</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; if self.page_queue.empty():</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; break</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; url = self.page_queue.get()</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; response = requests.get(url, headers=self.headers)</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; text = response.text</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; html = etree.HTML(text)</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; descs = html.xpath(&quot;//div[@class=&#39;j-r-list-c-desc&#39;]&quot;)</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; for desc in descs:</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; jokes = desc.xpath(&quot;.//text()&quot;)</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; joke = &quot;\\n&quot;.join(jokes).strip()</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; link = self.base_domain+desc.xpath(&quot;.//a/@href&quot;)[0]</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; self.joke_queue.put((joke,link))</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; print(&#39;=&#39;*30+&quot;第%s页下载完成！&quot;%url.split(&#39;/&#39;)[-1]+&quot;=&quot;*30)</p><p><br/></p><p>class BSWriter(threading.Thread):</p><p>&nbsp; &nbsp; headers = {</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &#39;User-Agent&#39;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36&#39;</p><p>&nbsp; &nbsp; }</p><p><br/></p><p>&nbsp; &nbsp; def __init__(self, joke_queue, writer,gLock, *args, **kwargs):</p><p>&nbsp; &nbsp; &nbsp; &nbsp; super(BSWriter, self).__init__(*args, **kwargs)</p><p>&nbsp; &nbsp; &nbsp; &nbsp; self.joke_queue = joke_queue</p><p>&nbsp; &nbsp; &nbsp; &nbsp; self.writer = writer</p><p>&nbsp; &nbsp; &nbsp; &nbsp; self.lock = gLock</p><p><br/></p><p>&nbsp; &nbsp; def run(self):</p><p>&nbsp; &nbsp; &nbsp; &nbsp; while True:</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; try:</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; joke_info = self.joke_queue.get(timeout=40)</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; joke,link = joke_info</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; self.lock.acquire()</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; self.writer.writerow((joke,link))</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; self.lock.release()</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; print(&#39;保存一条&#39;)</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; except:</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; break</p><p><br/></p><p>def main():</p><p>&nbsp; &nbsp; page_queue = Queue(10)</p><p>&nbsp; &nbsp; joke_queue = Queue(500)</p><p>&nbsp; &nbsp; gLock = threading.Lock()</p><p>&nbsp; &nbsp; fp = open(&#39;bsbdj.csv&#39;, &#39;a&#39;,newline=&#39;&#39;, encoding=&#39;utf-8&#39;)</p><p>&nbsp; &nbsp; writer = csv.writer(fp)</p><p>&nbsp; &nbsp; writer.writerow((&#39;content&#39;, &#39;link&#39;))</p><p><br/></p><p>&nbsp; &nbsp; for x in range(1,11):</p><p>&nbsp; &nbsp; &nbsp; &nbsp; url = &#39;http://www.budejie.com/text/%d&#39; % x</p><p>&nbsp; &nbsp; &nbsp; &nbsp; page_queue.put(url)</p><p><br/></p><p>&nbsp; &nbsp; for x in range(5):</p><p>&nbsp; &nbsp; &nbsp; &nbsp; t = BSSpider(page_queue,joke_queue)</p><p>&nbsp; &nbsp; &nbsp; &nbsp; t.start()</p><p><br/></p><p>&nbsp; &nbsp; for x in range(5):</p><p>&nbsp; &nbsp; &nbsp; &nbsp; t = BSWriter(joke_queue,writer,gLock)</p><p>&nbsp; &nbsp; &nbsp; &nbsp; t.start()</p><p><br/></p><p>if __name__ == &#39;__main__&#39;:</p><p>&nbsp; &nbsp; main()</p><p>```</p><p><br/></p>','2019-04-20'),(7,'协程','<p>## 协程&nbsp;&nbsp;</p><p><br/></p><p>&gt; go 语言先提出来的&nbsp; &nbsp; &nbsp;</p><p><br/></p><p>## 概念&nbsp;</p><p><br/></p><p>```</p><p>又叫微线程 比线程还要小的一个单元&nbsp; 这个单元是执行单元&nbsp; 它拥有CPU上下文&nbsp; &nbsp;适当的时候 我们可以从一个协程切换到另外一个协程&nbsp; 只要切换的过程中能够保存或者恢复上下文 那么程序就能够运行&nbsp;&nbsp;</p><p><br/></p><p>换句话说:&nbsp;</p><p>可以理解为线程中的一个函数&nbsp; &nbsp;可以再任何地方保存当前函数的临时变量信息&nbsp; &nbsp;切换到另外的函数 （不是调用函数）去执行&nbsp; &nbsp;什么时候切换不是由cpu 而是由开发者决定&nbsp; &nbsp; &nbsp;</p><p>```</p><p><br/></p><p><br/></p><p><br/></p><p>## 协程和线程的差异&nbsp;&nbsp;</p><p><br/></p><p>1. 协程执行效率高&nbsp; 子程序执行不用线程切换&nbsp; 而是由程序自身去控制&nbsp; &nbsp;这样减少了 线程的开销&nbsp; &nbsp;线程数量越多 协程的优势越明显&nbsp;&nbsp;</p><p>2. 不需要锁机制&nbsp; 因为协程就是一个线程来执行&nbsp; &nbsp;不存在共享变量的冲突&nbsp; 只需要判断状态 就好了&nbsp;&nbsp;</p><p>&nbsp; &nbsp;1. Lock()&nbsp; 互斥锁</p><p>&nbsp; &nbsp;2. RLock() 重复锁&nbsp; 锁可以被重用&nbsp; &nbsp;</p><p>3. 协程 利用不了多核&nbsp; CPU&nbsp; &nbsp;多线程+协程&nbsp; &nbsp;</p><p><br/></p><p><br/></p><p><br/></p><p>### greenlet+ switch 实现 协程调度&nbsp;&nbsp;</p><p><br/></p><p>```</p><p>from greenlet import greenlet</p><p>import time</p><p><br/></p><p>def f1():</p><p>&nbsp; &nbsp; print(&#39;男人最害怕哪一天?很明显1月31日&#39;)</p><p>&nbsp; &nbsp; time.sleep(2)</p><p><br/></p><p>&nbsp; &nbsp; print(&quot;种花多没意思,我们一起种草莓&quot;)</p><p>&nbsp; &nbsp; time.sleep(3)</p><p>&nbsp; &nbsp; g2.switch()</p><p>def f2():</p><p>&nbsp; &nbsp; print(&#39;2错过我过了这个村,我在下个村口等你&#39;)</p><p>&nbsp; &nbsp; time.sleep(2)</p><p>&nbsp; &nbsp; g1.switch() #把cpu的执行全交给 g1 协程</p><p><br/></p><p><br/></p><p>if __name__ == &quot;__main__&quot;:</p><p>&nbsp; &nbsp; g1 = greenlet(f1)</p><p>&nbsp; &nbsp; g2 = greenlet(f2)</p><p>&nbsp; &nbsp; g2.switch()</p><p>```</p><p><br/></p><p><br/></p><p><br/></p><p>## gevent&nbsp; 实现协程&nbsp;</p><p><br/></p><p>&gt; gevent&nbsp; +sleep&nbsp; &nbsp;优先将cpu的执行权交给未睡眠的 协程&nbsp; &nbsp;</p><p><br/></p><p>```</p><p>import gevent</p><p><br/></p><p>#gevent +sleep 谁睡眠时间短 或者此刻谁没睡眠&nbsp; 就把 cpu的执行权给哪个协程</p><p>def f1():</p><p>&nbsp; &nbsp; gevent.sleep(1)</p><p>&nbsp; &nbsp; print(&quot;协程1开始&quot;)</p><p>&nbsp; &nbsp; gevent.sleep(1)</p><p>&nbsp; &nbsp; print(&quot;协程1结束&quot;)</p><p>def f2():</p><p>&nbsp; &nbsp; gevent.sleep(3)</p><p>&nbsp; &nbsp; print(&quot;协程2开始&quot;)</p><p><br/></p><p>&nbsp; &nbsp; gevent.sleep(3)</p><p>&nbsp; &nbsp; print(&quot;协程2结束&quot;)</p><p><br/></p><p>def f3():</p><p>&nbsp; &nbsp; gevent.sleep(5)</p><p>&nbsp; &nbsp; print(&quot;协程3开始&quot;)</p><p><br/></p><p>&nbsp; &nbsp; gevent.sleep(5)</p><p>&nbsp; &nbsp; print(&quot;协程3结束&quot;)</p><p><br/></p><p>if __name__ == &quot;__main__&quot;:</p><p>&nbsp; &nbsp; g1 = gevent.spawn(f1)</p><p>&nbsp; &nbsp; g2 = gevent.spawn(f2)</p><p>&nbsp; &nbsp; g3 = gevent.spawn(f3)</p><p><br/></p><p>&nbsp; &nbsp; # g1.join()</p><p>&nbsp; &nbsp; # g2.join()</p><p>&nbsp; &nbsp; # g3.join()</p><p><br/></p><p>&nbsp; &nbsp; gevent.joinall([g1,g2,g3])</p><p>```</p><p><br/></p><p><br/></p><p><br/></p><p>作业:</p><p><br/></p><p>1.boss直聘 自己能写下来&nbsp;&nbsp;</p><p><br/></p><p>2.12306抢票软件逻辑弄明白&nbsp;</p><p><br/></p><p>3.多线程爬取链家&nbsp; &nbsp;&nbsp;</p><p><br/></p>','2019-04-20'),(8,'Scrapy框架架构','<p># Scrapy框架架构</p><p><br/></p><p>## Scrapy框架介绍：</p><p><br/></p><p>写一个爬虫，需要做很多的事情。比如：发送网络请求、数据解析、数据存储、反反爬虫机制（更换ip代理、设置请求头等）、异步请求等。这些工作如果每次都要自己从零开始写的话，比较浪费时间。因此`Scrapy`把一些基础的东西封装好了，在他上面写爬虫可以变的更加的高效（爬取效率和开发效率）。因此真正在公司里，一些上了量的爬虫，都是使用`Scrapy`框架来解决。</p><p><br/></p><p>## Scrapy架构图：</p><p><br/></p><p>1. 流程图（1）：</p><p>&nbsp; &nbsp;![scrapy_all](C:\\Users\\neyo\\Desktop\\scrapy\\scrapy_all.png)</p><p>2. 流程图（2）：</p><p>&nbsp; &nbsp;</p><p><br/></p><p>## Scrapy![框架图](C:\\Users\\neyo\\Desktop\\scrapy\\框架图.png)框架模块功能：</p><p><br/></p><p>1.爬虫发送请求 并不是马上发出去&nbsp; 而是引擎&nbsp;</p><p><br/></p><p>2.再 发给调度器 调度器接收到url&nbsp; 以后 将url生成requests对象 存储到队列中</p><p><br/></p><p>3.引擎从调度器种取出请求&nbsp;</p><p><br/></p><p>4.引擎将requests对象 扔给下载器&nbsp; &nbsp;</p><p><br/></p><p>5.下载器拿到请求从网上下载数据 再将数据 组装成response对象返回给引擎&nbsp;&nbsp;</p><p><br/></p><p>6.引擎拿到response对象 返回给爬虫&nbsp;&nbsp;</p><p><br/></p><p>7.爬虫对数据再进行分析&nbsp; 留下想要 的 数据 再返回给 引擎</p><p><br/></p><p>&nbsp;8.引擎再给管道&nbsp; &nbsp;存到 redis 或者mysql 或者 mongodb中</p><p><br/></p><p>&nbsp;引擎和下载器&nbsp; 之间 有中间件&nbsp; &nbsp; 爬虫和 引擎之间 也有中间件&nbsp; &nbsp;</p><p><br/></p><p>1. `Scrapy Engine（引擎）`：`Scrapy`框架的核心部分。负责在`Spider`和`ItemPipeline`、`Downloader`、`Scheduler`中间通信、传递数据等。类似于汽车发动机</p><p>2. `Spider（爬虫）`：发送需要爬取的链接给引擎，最后引擎把其他模块请求回来的数据再发送给爬虫，爬虫就去解析想要的数据。这个部分是我们开发者自己写的，因为要爬取哪些链接，页面中的哪些数据是我们需要的，都是由程序员自己决定。</p><p>3. `Scheduler（调度器）`：负责接收引擎发送过来的请求，并按照一定的方式进行排列和整理，负责调度请求的顺序等。</p><p>4. `Downloader（下载器）`：负责接收引擎传过来的下载请求，然后去网络上下载对应的数据再交还给引擎。</p><p>5. `Item Pipeline（管道）`：负责将`Spider（爬虫）`传递过来的数据进行保存。具体保存在哪里，应该看开发者自己的需求。</p><p>6. `Downloader Middlewares（下载中间件）`：可以扩展下载器和引擎之间通信功能的中间件。</p><p>7. `Spider Middlewares（Spider中间件）`：可以扩展引擎和爬虫之间通信功能的中间件。</p><p><br/></p><p># Scrapy快速入门</p><p><br/></p><p>## 安装和文档：</p><p><br/></p><p>1. 安装：通过`pip install scrapy`即可安装。</p><p>2. Scrapy官方文档：&lt;http://doc.scrapy.org/en/latest&gt;</p><p>3. Scrapy中文文档：&lt;http://scrapy-chs.readthedocs.io/zh_CN/latest/index.html&gt;</p><p><br/></p><p>&gt; 注意：</p><p>&gt;</p><p>&gt; 1. 在`ubuntu`上安装`scrapy`之前，需要先安装以下依赖：</p><p>&gt;&nbsp; &nbsp;`sudo apt-get install python3-dev build-essential python3-pip libxml2-dev libxslt1-dev zlib1g-dev libffi-dev libssl-dev`，然后再通过`pip install scrapy`安装。</p><p>&gt; 2. 如果在`windows`系统下，提示这个错误`ModuleNotFoundError: No module named &#39;win32api&#39;`，那么使用以下命令可以解决：`pip install pypiwin32`。</p><p>&gt; 3. 下载 Twisted-18.9.0-cp36-cp36m-win_amd64.whl 然后放到指定的目录下 纯英文 没权限限制&nbsp; &nbsp;切换到这个目录&nbsp; pip install Twisted-18.9.0-cp36-cp36m-win_amd64.whl&nbsp; &nbsp;&nbsp;</p><p>&gt; 4. pip install scrapy&nbsp;&nbsp;</p><p><br/></p><p>## 快速入门：</p><p><br/></p><p>### 创建项目：</p><p><br/></p><p>要使用`Scrapy`框架创建项目，需要通过命令来创建。首先进入到你想把这个项目存放的目录。然后使用以下命令创建：</p><p><br/></p><p>```</p><p>scrapy startproject [项目名称]</p><p>```</p><p><br/></p><p>### 目录结构介绍：</p><p><br/></p><p>![目录结构](C:\\Users\\neyo\\Desktop\\scrapy\\目录结构.png)</p><p><br/></p><p><br/></p><p>&nbsp; &nbsp;以下介绍下主要文件的作用：</p><p><br/></p><p>&nbsp; &nbsp; &nbsp; 1. items.py：用来存放爬虫爬取下来数据的模型。</p><p>&nbsp; &nbsp; &nbsp; 2. middlewares.py：用来存放各种中间件的文件。</p><p>&nbsp; &nbsp; &nbsp; 3. pipelines.py：用来将`items`的模型存储到本地磁盘中。</p><p>&nbsp; &nbsp; &nbsp; 4. settings.py：本爬虫的一些配置信息（比如请求头、多久发送一次请求、ip代理池等）。</p><p>&nbsp; &nbsp; &nbsp; 5. scrapy.cfg：项目的配置文件。</p><p>&nbsp; &nbsp; &nbsp; 6. spiders包：以后所有的爬虫，都是存放到这个里面。</p><p><br/></p><p><br/></p><p><br/></p><p>## 当引擎将下载组装的respons对象给 爬虫的时候&nbsp; &nbsp;</p><p><br/></p><p>爬虫对数据进行分析&nbsp;&nbsp;</p><p><br/></p><p>response.xpath() 详情 ctrl+鼠标点击 xpath 查看 其它的分析方法&nbsp; &nbsp;返回的内容是 SelectorList&nbsp;</p><p><br/></p><p>一下两个都是将其转成Unicode编码&nbsp; 并提取出来</p><p><br/></p><p>get() 返回的是Selector 中的第一个文本&nbsp;&nbsp;</p><p><br/></p><p>getall()返回的是Selector 中的所有文本 是个列表</p><p><br/></p><p><br/></p><p><br/></p><p>&nbsp; &nbsp;### 使用Scrapy框架爬取糗事百科段子：</p><p><br/></p><p>&nbsp; &nbsp;#### 使用命令创建一个爬虫：</p><p><br/></p><p>&nbsp; &nbsp;```</p><p>&nbsp; &nbsp;scrapy genspider qsbk &quot;qiushibaike.com&quot;</p><p>&nbsp; &nbsp;```</p><p><br/></p><p>&nbsp; &nbsp;创建了一个名字叫做`qsbk`的爬虫，并且能爬取的网页只会限制在`qiushibaike.com`这个域名下。</p><p><br/></p><p>&nbsp; &nbsp;#### 爬虫代码解析：</p><p><br/></p><p>&nbsp; &nbsp;```</p><p>&nbsp; &nbsp;import scrapy</p><p>&nbsp; &nbsp;</p><p>&nbsp; &nbsp;class QsbkSpider(scrapy.Spider):</p><p>&nbsp; &nbsp; &nbsp; &nbsp;name = &#39;qsbk&#39;</p><p>&nbsp; &nbsp; &nbsp; &nbsp;allowed_domains = [&#39;qiushibaike.com&#39;]</p><p>&nbsp; &nbsp; &nbsp; &nbsp;start_urls = [&#39;http://qiushibaike.com/&#39;]</p><p>&nbsp; &nbsp;</p><p>&nbsp; &nbsp; &nbsp; &nbsp;def parse(self, response):</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;pass</p><p>&nbsp; &nbsp;```</p><p><br/></p><p>&nbsp; &nbsp;其实这些代码我们完全可以自己手动去写，而不用命令。只不过是不用命令，自己写这些代码比较麻烦。</p><p>&nbsp; &nbsp;要创建一个Spider，那么必须自定义一个类，继承自`scrapy.Spider`，然后在这个类中定义三个属性和一个方法。</p><p><br/></p><p>&nbsp; &nbsp;1. name：这个爬虫的名字，名字必须是唯一的。</p><p>&nbsp; &nbsp;2. allow_domains：允许的域名。爬虫只会爬取这个域名下的网页，其他不是这个域名下的网页会被自动忽略。</p><p>&nbsp; &nbsp;3. start_urls：爬虫从这个变量中的url开始。</p><p>&nbsp; &nbsp;4. parse：引擎会把下载器下载回来的数据扔给爬虫解析，爬虫再把数据传给这个`parse`方法。这个是个固定的写法。这个方法的作用有两个，第一个是提取想要的数据。第二个是生成下一个请求的url。</p><p><br/></p><p>&nbsp; &nbsp;#### 修改`settings.py`代码：</p><p><br/></p><p>&nbsp; &nbsp;在做一个爬虫之前，一定要记得修改`setttings.py`中的设置。两个地方是强烈建议设置的。</p><p><br/></p><p>&nbsp; &nbsp;1. `ROBOTSTXT_OBEY`设置为False。默认是True。即遵守机器协议，那么在爬虫的时候，scrapy首先去找robots.txt文件，如果没有找到。则直接停止爬取。</p><p>&nbsp; &nbsp;2. `DEFAULT_REQUEST_HEADERS`添加`User-Agent`。这个也是告诉服务器，我这个请求是一个正常的请求，不是一个爬虫。</p><p><br/></p><p>&nbsp; &nbsp;#### 完成的爬虫代码：</p><p><br/></p><p>&nbsp; &nbsp;1. 爬虫部分代码：</p><p><br/></p><p>&nbsp; &nbsp; &nbsp; ```</p><p>&nbsp; &nbsp; &nbsp; &nbsp;import scrapy</p><p>&nbsp; &nbsp; &nbsp; &nbsp;from abcspider.items import QsbkItem</p><p>&nbsp; &nbsp; &nbsp;&nbsp;</p><p>&nbsp; &nbsp; &nbsp; &nbsp;class QsbkSpider(scrapy.Spider):</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;name = &#39;qsbk&#39;</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;allowed_domains = [&#39;qiushibaike.com&#39;]</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;start_urls = [&#39;https://www.qiushibaike.com/text/&#39;]</p><p>&nbsp; &nbsp; &nbsp;&nbsp;</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;def parse(self, response):</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;outerbox = response.xpath(&quot;//div[@id=&#39;content-left&#39;]/div&quot;)</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;items = []</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;for box in outerbox:</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;author = box.xpath(&quot;.//div[contains(@class,&#39;author&#39;)]//h2/text()&quot;).extract_first().strip()</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;content = box.xpath(&quot;.//div[@class=&#39;content&#39;]/span/text()&quot;).extract_first().strip()</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;item = QsbkItem()</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;item[&quot;author&quot;] = author</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;item[&quot;content&quot;] = content</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;items.append(item)</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;return items</p><p>&nbsp; &nbsp; &nbsp; ```</p><p><br/></p><p>&nbsp; &nbsp;2. items.py部分代码：</p><p><br/></p><p>&nbsp; &nbsp; &nbsp; ```</p><p>&nbsp; &nbsp; &nbsp; &nbsp;import scrapy</p><p>&nbsp; &nbsp; &nbsp; &nbsp;class QsbkItem(scrapy.Item):</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;author = scrapy.Field()</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;content = scrapy.Field()</p><p>&nbsp; &nbsp; &nbsp; ```</p><p><br/></p><p>&nbsp; &nbsp;3. pipeline部分代码：</p><p><br/></p><p>&nbsp; &nbsp; &nbsp; ```</p><p>&nbsp; &nbsp; &nbsp; &nbsp;import json</p><p>&nbsp; &nbsp; &nbsp;&nbsp;</p><p>&nbsp; &nbsp; &nbsp; &nbsp;class AbcspiderPipeline(object):</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;def __init__(self):</p><p>&nbsp; &nbsp; &nbsp;&nbsp;</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;self.items = []</p><p>&nbsp; &nbsp; &nbsp;&nbsp;</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;def process_item(self, item, spider):</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;self.items.append(dict(item))</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;print(&quot;=&quot;*40)</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;return item</p><p>&nbsp; &nbsp; &nbsp;&nbsp;</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;def close_spider(self,spider):</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;with open(&#39;qsbk.json&#39;,&#39;w&#39;,encoding=&#39;utf-8&#39;) as fp:</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;json.dump(self.items,fp,ensure_ascii=False)</p><p>&nbsp; &nbsp; &nbsp; ```</p><p><br/></p><p>&nbsp; &nbsp;#### 运行scrapy项目：</p><p><br/></p><p>&nbsp; &nbsp;运行scrapy项目。需要在终端，进入项目所在的路径，然后`scrapy crawl [爬虫名字]`即可运行指定的爬虫。如果不想每次都在命令行中运行，那么可以把这个命令写在一个文件中。以后就在pycharm中执行运行这个文件就可以了。比如现在新创建一个文件叫做`start.py`，然后在这个文件中填入以下代码：</p><p><br/></p><p>&nbsp; &nbsp;```</p><p>&nbsp; &nbsp;from scrapy import cmdline</p><p>&nbsp; &nbsp;</p><p>&nbsp; &nbsp;cmdline.execute(&quot;scrapy crawl qsbk&quot;.split())</p><p>&nbsp; &nbsp;```</p><p><br/></p><p><br/></p><p><br/></p><p>## pipline 管道 用来将数据存储在文件或者数据库中&nbsp; 有三个方法是常用的&nbsp; &nbsp;&nbsp;</p><p><br/></p><p>1. open_spider(self,spider) #当爬虫被打开的时候执行&nbsp;&nbsp;</p><p><br/></p><p>2. ```</p><p>&nbsp; &nbsp;process_item(self, item, spider)当爬虫 有item 传递过来的时候 调用&nbsp;&nbsp;</p><p>&nbsp; &nbsp;```</p><p><br/></p><p>3.&nbsp; close_spider(self,spider) #当爬虫关闭的时候调用</p><p><br/></p><p><br/></p><p><br/></p><p><br/></p><p><br/></p><p>### scrapy 导出器&nbsp; JsonItemExporter 、JsonLinesItemExporter</p><p><br/></p><p>1.JsonItemExporter&nbsp;&nbsp;</p><p><br/></p><p>&gt; 每次把数据添加到内存中 最后统一写到磁盘中&nbsp; &nbsp;好处存储的是一个满足json规则的数据&nbsp; 缺点：数据量大&nbsp; 耗内存&nbsp;&nbsp;</p><p><br/></p><p>```</p><p>from&nbsp; scrapy.exporters import JsonItemExporter,JsonLinesItemExporter</p><p>class QsbkPipeline(object):</p><p>&nbsp; &nbsp; def __init__(self):</p><p>&nbsp; &nbsp; &nbsp; &nbsp; self.fp = open(&quot;qsbk.json&quot;,&#39;wb&#39;)</p><p>&nbsp; &nbsp; &nbsp; &nbsp; self.exporter = JsonItemExporter(self.fp,ensure_ascii=False,encoding=&#39;utf-8&#39;)</p><p>&nbsp; &nbsp; &nbsp; &nbsp; self.exporter.start_exporting()</p><p>&nbsp; &nbsp; def open_spider(self,spider):</p><p>&nbsp; &nbsp; &nbsp; &nbsp; print(&quot;爬虫开始了......&quot;)</p><p>&nbsp; &nbsp; def process_item(self, item, spider):</p><p>&nbsp; &nbsp; &nbsp; &nbsp; self.exporter.export_item(item)</p><p>&nbsp; &nbsp; &nbsp; &nbsp; return item</p><p><br/></p><p>&nbsp; &nbsp; def close_spider(self,spider):</p><p>&nbsp; &nbsp; &nbsp; &nbsp; self.fp.close()</p><p>&nbsp; &nbsp; &nbsp; &nbsp; print(&quot;爬虫结束了......&quot;)</p><p>```</p><p><br/></p><p>2.JsonLinesItemExporter</p><p><br/></p><p>&gt; 每次调用export_item 存到磁盘中 好处 不耗内存&nbsp; 直接持久化 安全&nbsp; 坏处&nbsp; 是每个字典是一行 整个文件不满足json规则&nbsp; &nbsp;</p><p><br/></p><p>```</p><p>from&nbsp; scrapy.exporters import JsonLinesItemExporter</p><p>class QsbkPipeline(object):</p><p>&nbsp; &nbsp; def __init__(self):</p><p>&nbsp; &nbsp; &nbsp; &nbsp; self.fp = open(&quot;duanzi.json&quot;,&#39;wb&#39;)</p><p>&nbsp; &nbsp; &nbsp; &nbsp; self.exporter = JsonLinesItemExporter(self.fp,ensure_ascii=False,encoding=&#39;utf-8&#39;)</p><p>&nbsp; &nbsp; def open_spider(self,spider):</p><p>&nbsp; &nbsp; &nbsp; &nbsp; print(&quot;爬虫开始了......&quot;)</p><p>&nbsp; &nbsp; def process_item(self, item, spider):</p><p>&nbsp; &nbsp; &nbsp; &nbsp; self.exporter.export_item(item)</p><p>&nbsp; &nbsp; &nbsp; &nbsp; return item</p><p><br/></p><p>&nbsp; &nbsp; def close_spider(self,spider):</p><p>&nbsp; &nbsp; &nbsp; &nbsp; self.fp.close()</p><p>&nbsp; &nbsp; &nbsp; &nbsp; print(&quot;爬虫结束了......&quot;)</p><p>```</p><p><br/></p><p><br/></p><p><br/></p><p>### 糗事百科 分页&nbsp;&nbsp;</p><p><br/></p><p>```</p><p>&nbsp;next_url = response.xpath(&quot;//ul[@class=&#39;pagination&#39;]/li[last()]/a/@href&quot;).get()</p><p>&nbsp; &nbsp; &nbsp; &nbsp; if not next_url:</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; return</p><p>&nbsp; &nbsp; &nbsp; &nbsp; else:</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; yield scrapy.Request(self.base_domain+next_url,callback=self.parse)</p><p>```</p><p><br/></p><p><br/></p><p><br/></p><p><br/></p><p><br/></p><p># CrawlSpider</p><p><br/></p><p>在上一个糗事百科的爬虫案例中。我们是自己在解析完整个页面后获取下一页的url，然后重新发送一个请求。有时候我们想要这样做，只要满足某个条件的url，都给我进行爬取。那么这时候我们就可以通过`CrawlSpider`来帮我们完成了。`CrawlSpider`继承自`Spider`，只不过是在之前的基础之上增加了新的功能，可以定义爬取的url的规则，以后scrapy碰到满足条件的url都进行爬取，而不用手动的`yield Request`。</p><p><br/></p><p>## CrawlSpider爬虫：</p><p><br/></p><p>### 创建CrawlSpider爬虫：</p><p><br/></p><p>之前创建爬虫的方式是通过`scrapy genspider [爬虫名字] [域名]`的方式创建的。如果想要创建`CrawlSpider`爬虫，那么应该通过以下命令创建：</p><p><br/></p><p>```</p><p>scrapy genspider&nbsp; [爬虫名字] [域名] #原来是这样子&nbsp;&nbsp;</p><p>scrapy genspider -t crawl [爬虫名字] [域名]</p><p>```</p><p><br/></p><p>### LinkExtractors链接提取器：</p><p><br/></p><p>使用`LinkExtractors`可以不用程序员自己提取想要的url，然后发送请求。这些工作都可以交给`LinkExtractors`，他会在所有爬的页面中找到满足规则的`url`，实现自动的爬取。以下对`LinkExtractors`类做一个简单的介绍：</p><p><br/></p><p>```</p><p>class scrapy.linkextractors.LinkExtractor(</p><p>&nbsp; &nbsp; allow = (),</p><p>&nbsp; &nbsp; deny = (),</p><p>&nbsp; &nbsp; allow_domains = (),</p><p>&nbsp; &nbsp; deny_domains = (),</p><p>&nbsp; &nbsp; deny_extensions = None,</p><p>&nbsp; &nbsp; restrict_xpaths = (),</p><p>&nbsp; &nbsp; tags = (&#39;a&#39;,&#39;area&#39;),</p><p>&nbsp; &nbsp; attrs = (&#39;href&#39;),</p><p>&nbsp; &nbsp; canonicalize = True,</p><p>&nbsp; &nbsp; unique = True,</p><p>&nbsp; &nbsp; process_value = None</p><p>)</p><p>```</p><p><br/></p><p>主要参数讲解：</p><p><br/></p><p>- allow：允许的url。所有满足这个正则表达式的url都会被提取。</p><p>- deny：禁止的url。所有满足这个正则表达式的url都不会被提取。</p><p>- allow_domains：允许的域名。只有在这个里面指定的域名的url才会被提取。</p><p>- deny_domains：禁止的域名。所有在这个里面指定的域名的url都不会被提取。</p><p>- restrict_xpaths：严格的xpath。和allow共同过滤链接。</p><p><br/></p><p>### Rule规则类：</p><p><br/></p><p>定义爬虫的规则类。以下对这个类做一个简单的介绍：</p><p><br/></p><p>```</p><p>class scrapy.spiders.Rule(</p><p>&nbsp; &nbsp; link_extractor,&nbsp;</p><p>&nbsp; &nbsp; callback = None,&nbsp;</p><p>&nbsp; &nbsp; cb_kwargs = None,&nbsp;</p><p>&nbsp; &nbsp; follow = None,&nbsp;</p><p>&nbsp; &nbsp; process_links = None,&nbsp;</p><p>&nbsp; &nbsp; process_request = None</p><p>)</p><p>```</p><p><br/></p><p>主要参数讲解：</p><p><br/></p><p>- link_extractor：一个`LinkExtractor`对象，用于定义爬取规则。</p><p>- callback：满足这个规则的url，应该要执行哪个回调函数。因为`CrawlSpider`使用了`parse`作为回调函数，因此不要覆盖`parse`作为回调函数自己的回调函数。</p><p>- follow：指定根据该规则从response中提取的链接是否需要跟进。</p><p>- process_links：从link_extractor中获取到链接后会传递给这个函数，用来过滤不需要爬取的链接。</p><p><br/></p><p><br/></p><p><br/></p><p># Scrapy Shell</p><p><br/></p><p>我们想要在爬虫中使用xpath、beautifulsoup、正则表达式、css选择器等来提取想要的数据。但是因为`scrapy`是一个比较重的框架。每次运行起来都要等待一段时间。因此要去验证我们写的提取规则是否正确，是一个比较麻烦的事情。因此`Scrapy`提供了一个shell，用来方便的测试规则。当然也不仅仅局限于这一个功能。</p><p><br/></p><p>## 打开Scrapy Shell：</p><p><br/></p><p>打开cmd终端，进入到`Scrapy`项目所在的目录，然后进入到`scrapy`框架所在的虚拟环境中，输入命令`scrapy shell [链接]`。就会进入到scrapy的shell环境中。在这个环境中，你可以跟在爬虫的`parse`方法中一样使用了。</p><p><br/></p><p><br/></p><p><br/></p><p># Request和Response对象</p><p><br/></p><p>## Request对象：</p><p><br/></p><p>Request对象在我们写爬虫，爬取一页的数据需要重新发送一个请求的时候调用。这个类需要传递一些参数，其中比较常用的参数有：</p><p><br/></p><p>1. `url`：这个request对象发送请求的url。</p><p>2. `callback`：在下载器下载完相应的数据后执行的回调函数。</p><p>3. `method`：请求的方法。默认为`GET`方法，可以设置为其他方法。</p><p>4. `headers`：请求头，对于一些固定的设置，放在`settings.py`中指定就可以了。对于那些非固定的，可以在发送请求的时候指定。</p><p>5. `meta`：比较常用。用于在不同的请求之间传递数据用的。</p><p>6. `encoding`：编码。默认的为`utf-8`，使用默认的就可以了。</p><p>7. `dont_filter`：表示不由调度器过滤。在执行多次重复的请求的时候用得比较多。</p><p>8. `errback`：在发生错误的时候执行的函数。</p><p><br/></p><p>## Response对象：</p><p><br/></p><p>Response对象一般是由`Scrapy`给你自动构建的。因此开发者不需要关心如何创建`Response`对象，而是如何使用他。`Response`对象有很多属性，可以用来提取数据的。主要有以下属性：</p><p><br/></p><p>1. meta：从其他请求传过来的`meta`属性，可以用来保持多个请求之间的数据连接。</p><p>2. encoding：返回当前字符串编码和解码的格式。</p><p>3. text：将返回来的数据作为`unicode`字符串返回。</p><p>4. body：将返回来的数据作为`bytes`字符串返回。</p><p>5. xpath：xapth选择器。</p><p>6. css：css选择器。</p><p><br/></p><p>## 发送POST请求：</p><p><br/></p><p>有时候我们想要在请求数据的时候发送post请求，那么这时候需要使用`Request`的子类`FormRequest`来实现。如果想要在爬虫一开始的时候就发送`POST`请求，那么需要在爬虫类中重写`start_requests(self)`方法，并且不再调用`start_urls`里的url。</p><p><br/></p>','2019-04-20'),(9,'redis教程：','<p># redis教程：</p><p><br/></p><p>## 概述</p><p><br/></p><p>`redis`是一种支持分布式的`nosql`数据库,他的数据是保存在内存中，同时`redis`可以定时把内存数据同步到磁盘，即可以将数据持久化，并且他比`memcached`支持更多的数据结构(`string`,`list列表[队列和栈]`,`set[集合]`,`sorted set[有序集合]`,`hash(hash表)`)。相关参考文档：&lt;http://redisdoc.com/index.html&gt;</p><p><br/></p><p>## redis使用场景：</p><p><br/></p><p>1. 登录会话存储：存储在`redis`中，与`memcached`相比，数据不会丢失。</p><p>2. 排行版/计数器：比如一些秀场类的项目，经常会有一些前多少名的主播排名。还有一些文章阅读量的技术，或者新浪微博的点赞数等。</p><p>3. 作为消息队列：比如`celery`就是使用`redis`作为中间人。</p><p>4. 当前在线人数：还是之前的秀场例子，会显示当前系统有多少在线人数。</p><p>5. 一些常用的数据缓存：比如我们的`BBS`论坛，板块不会经常变化的，但是每次访问首页都要从`mysql`中获取，可以在`redis`中缓存起来，不用每次请求数据库。</p><p>6. 把前200篇文章缓存或者评论缓存：一般用户浏览网站，只会浏览前面一部分文章或者评论，那么可以把前面200篇文章和对应的评论缓存起来。用户访问超过的，就访问数据库，并且以后文章超过200篇，则把之前的文章删除。</p><p>7. 好友关系：微博的好友关系使用`redis`实现。</p><p>8. 发布和订阅功能：可以用来做聊天软件。</p><p><br/></p><p>## `redis`和`memcached`的比较：</p><p><br/></p><p>|&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | memcached&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| redis&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |</p><p>| ------------ | ----------------------------- | ------------------------------ |</p><p>| 类型&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| 纯内存数据库&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | 内存磁盘同步数据库&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;|</p><p>| 数据类型&nbsp; &nbsp; &nbsp;| 在定义value时就要固定数据类型 | 不需要&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;|</p><p>| 虚拟内存&nbsp; &nbsp; &nbsp;| 不支持&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | 支持&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;|</p><p>| 过期策略&nbsp; &nbsp; &nbsp;| 支持&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | 支持&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;|</p><p>| 存储数据安全 | 不支持&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | 可以将数据同步到dump.db中&nbsp; &nbsp; &nbsp; |</p><p>| 灾难恢复&nbsp; &nbsp; &nbsp;| 不支持&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | 可以将磁盘中的数据恢复到内存中 |</p><p>| 分布式&nbsp; &nbsp; &nbsp; &nbsp;| 支持&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | 主从同步&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;|</p><p>| 订阅与发布&nbsp; &nbsp;| 不支持&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | 支持&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;|</p><p><br/></p><p>## `redis`在`ubuntu`系统中的安装与启动</p><p><br/></p><p>1. 安装：</p><p><br/></p><p>&nbsp; &nbsp;```</p><p>&nbsp; &nbsp; sudo apt-get install redis-server</p><p>&nbsp; &nbsp;```</p><p><br/></p><p>2. 卸载：</p><p><br/></p><p>&nbsp; &nbsp;```</p><p>&nbsp; &nbsp; sudo apt-get purge --auto-remove redis-server</p><p>&nbsp; &nbsp;```</p><p><br/></p><p>3. 启动：`redis`安装后，默认会自动启动，可以通过以下命令查看：</p><p><br/></p><p>&nbsp; &nbsp;```</p><p>&nbsp; &nbsp; ps aux|grep redis</p><p>&nbsp; &nbsp;```</p><p><br/></p><p>&nbsp; &nbsp;如果想自己手动启动，可以通过以下命令进行启动：</p><p><br/></p><p>&nbsp; &nbsp;```</p><p>&nbsp; &nbsp; sudo service redis-server start</p><p>&nbsp; &nbsp;```</p><p><br/></p><p>4. 停止：</p><p><br/></p><p>&nbsp; &nbsp;```</p><p>&nbsp; &nbsp; sudo service redis-server stop</p><p>&nbsp; &nbsp;```</p><p><br/></p><p>## redis在windows系统中的安装与启动：</p><p><br/></p><p>1. 下载：redis官方是不支持windows操作系统的。但是微软的开源部门将redis移植到了windows上。因此下载地址不是在redis官网上。而是在github上：&lt;https://github.com/MicrosoftArchive/redis/releases。&gt;</p><p>2. 安装：点击一顿下一步安装就可以了。</p><p>3. 运行：进入到`redis`安装所在的路径然后执行`redis-server.exe redis.windows.conf`就可以运行了。</p><p>4. 连接：`redis`和`mysql`以及`mongo`是一样的，都提供了一个客户端进行连接。输入命令`redis-cli`（前提是redis安装路径已经加入到环境变量中了）就可以连接到`redis`服务器了。</p><p><br/></p><p>## 其他机器访问本机redis服务器：</p><p><br/></p><p>想要让其他机器访问本机的redis服务器。那么要修改redis.conf的配置文件，将bind改成`bind [自己的ip地址或者0.0.0.0]`，其他机器才能访问。</p><p>**注意：bind绑定的是本机网卡的ip地址，而不是想让其他机器连接的ip地址。如果有多块网卡，那么可以绑定多个网卡的ip地址。如果绑定到额是0.0.0.0，那么意味着其他机器可以通过本机所有的ip地址进行访问。**</p><p><br/></p><p>## 对`redis`的操作</p><p><br/></p><p>对`redis`的操作可以用两种方式，第一种方式采用`redis-cli`，第二种方式采用编程语言，比如`Python`、`PHP`和`JAVA`等。</p><p><br/></p><p>1. 使用`redis-cli`对`redis`进行字符串操作：</p><p><br/></p><p>2. 启动`redis`：</p><p><br/></p><p>&nbsp; &nbsp;```</p><p>&nbsp; &nbsp; &nbsp;sudo service redis-server start</p><p>&nbsp; &nbsp;```</p><p><br/></p><p>3. 连接上</p><p><br/></p><p>&nbsp; &nbsp;```</p><p>&nbsp; &nbsp;redis-server</p><p>&nbsp; &nbsp;```</p><p><br/></p><p>&nbsp; &nbsp;：</p><p><br/></p><p>&nbsp; &nbsp;```</p><p>&nbsp; &nbsp; &nbsp;redis-cli -h [ip] -p [端口]</p><p>&nbsp; &nbsp;```</p><p><br/></p><p>4. 添加：</p><p><br/></p><p>&nbsp; &nbsp;```</p><p>&nbsp; &nbsp; &nbsp;set key value</p><p>&nbsp; &nbsp; &nbsp;如：</p><p>&nbsp; &nbsp; &nbsp;set username xiaotuo</p><p>&nbsp; &nbsp;```</p><p><br/></p><p>&nbsp; &nbsp;将字符串值`value`关联到`key`。如果`key`已经持有其他值，`set`命令就覆写旧值，无视其类型。并且默认的过期时间是永久，即永远不会过期。</p><p><br/></p><p>5. 删除：</p><p><br/></p><p>&nbsp; &nbsp;```</p><p>&nbsp; &nbsp; &nbsp;del key</p><p>&nbsp; &nbsp; &nbsp;如：</p><p>&nbsp; &nbsp; &nbsp;del username</p><p>&nbsp; &nbsp;```</p><p><br/></p><p>6. 设置过期时间：</p><p><br/></p><p>&nbsp; &nbsp;```</p><p>&nbsp; &nbsp; &nbsp;expire key timeout(单位为秒)</p><p>&nbsp; &nbsp;```</p><p><br/></p><p>&nbsp; &nbsp;也可以在设置值的时候，一同指定过期时间：</p><p><br/></p><p>&nbsp; &nbsp;```</p><p>&nbsp; &nbsp; &nbsp;set key value EX timeout</p><p>&nbsp; &nbsp; &nbsp;或：</p><p>&nbsp; &nbsp; &nbsp;setex key timeout value</p><p>&nbsp; &nbsp;```</p><p><br/></p><p>7. 查看过期时间：</p><p><br/></p><p>&nbsp; &nbsp;```</p><p>&nbsp; &nbsp; &nbsp;ttl key</p><p>&nbsp; &nbsp; &nbsp;如：</p><p>&nbsp; &nbsp; &nbsp;ttl username</p><p>&nbsp; &nbsp;```</p><p><br/></p><p>8. 查看当前`redis`中的所有`key`：</p><p><br/></p><p>&nbsp; &nbsp;```</p><p>&nbsp; &nbsp; &nbsp;keys *</p><p>&nbsp; &nbsp;```</p><p><br/></p><p>9. 列表操作：</p><p><br/></p><p>&nbsp; &nbsp;- 在列表左边添加元素：</p><p><br/></p><p>&nbsp; &nbsp; &nbsp;```</p><p>&nbsp; &nbsp; &nbsp; &nbsp;lpush key value</p><p>&nbsp; &nbsp; &nbsp;```</p><p><br/></p><p>&nbsp; &nbsp; &nbsp;将值`value`插入到列表`key`的表头。如果`key`不存在，一个空列表会被创建并执行`lpush`操作。当`key`存在但不是列表类型时，将返回一个错误。</p><p><br/></p><p>&nbsp; &nbsp;- 在列表右边添加元素：</p><p><br/></p><p>&nbsp; &nbsp; &nbsp;```</p><p>&nbsp; &nbsp; &nbsp; &nbsp;rpush key value</p><p>&nbsp; &nbsp; &nbsp;```</p><p><br/></p><p>&nbsp; &nbsp; &nbsp;将值value插入到列表key的表尾。如果key不存在，一个空列表会被创建并执行RPUSH操作。当key存在但不是列表类型时，返回一个错误。</p><p><br/></p><p>&nbsp; &nbsp;- 查看列表中的元素：</p><p><br/></p><p>&nbsp; &nbsp; &nbsp;```</p><p>&nbsp; &nbsp; &nbsp; &nbsp;lrange key start stop</p><p>&nbsp; &nbsp; &nbsp;```</p><p><br/></p><p>&nbsp; &nbsp; &nbsp;返回列表`key`中指定区间内的元素，区间以偏移量`start`和`stop`指定,如果要左边的第一个到最后的一个`lrange key 0 -1`。</p><p><br/></p><p>&nbsp; &nbsp;- 移除列表中的元素：</p><p><br/></p><p>&nbsp; &nbsp; &nbsp;- 移除并返回列表</p><p><br/></p><p>&nbsp; &nbsp; &nbsp; &nbsp;```</p><p>&nbsp; &nbsp; &nbsp; &nbsp;key</p><p>&nbsp; &nbsp; &nbsp; &nbsp;```</p><p><br/></p><p>&nbsp; &nbsp; &nbsp; &nbsp;的头元素：</p><p><br/></p><p>&nbsp; &nbsp; &nbsp; &nbsp;```</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;lpop key</p><p>&nbsp; &nbsp; &nbsp; &nbsp;```</p><p><br/></p><p>&nbsp; &nbsp; &nbsp;- 移除并返回列表的尾元素：</p><p><br/></p><p>&nbsp; &nbsp; &nbsp; &nbsp;```</p><p>&nbsp; &nbsp; &nbsp; &nbsp;rpop key</p><p>&nbsp; &nbsp; &nbsp; &nbsp;```</p><p><br/></p><p>&nbsp; &nbsp; &nbsp;- 移除并返回列表`key`的中间元素：</p><p><br/></p><p>&nbsp; &nbsp; &nbsp; &nbsp;```</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;lrem key count value</p><p>&nbsp; &nbsp; &nbsp; &nbsp;```</p><p><br/></p><p>&nbsp; &nbsp; &nbsp; &nbsp;将删除`key`这个列表中，`count`个值为`value`的元素。</p><p><br/></p><p>&nbsp; &nbsp;- 指定返回第几个元素：</p><p><br/></p><p>&nbsp; &nbsp; &nbsp;```</p><p>&nbsp; &nbsp; &nbsp; &nbsp;lindex key index</p><p>&nbsp; &nbsp; &nbsp;```</p><p><br/></p><p>&nbsp; &nbsp; &nbsp;将返回`key`这个列表中，索引为`index`的这个元素。</p><p><br/></p><p>&nbsp; &nbsp;- 获取列表中的元素个数：</p><p><br/></p><p>&nbsp; &nbsp; &nbsp;```</p><p>&nbsp; &nbsp; &nbsp; &nbsp;llen key</p><p>&nbsp; &nbsp; &nbsp; &nbsp;如：</p><p>&nbsp; &nbsp; &nbsp; &nbsp;llen languages</p><p>&nbsp; &nbsp; &nbsp;```</p><p><br/></p><p>&nbsp; &nbsp;- 删除指定的元素：</p><p><br/></p><p>&nbsp; &nbsp; &nbsp;```</p><p>&nbsp; &nbsp; &nbsp; &nbsp;lrem key count value</p><p>&nbsp; &nbsp; &nbsp; &nbsp;如：</p><p>&nbsp; &nbsp; &nbsp; &nbsp;lrem languages 0 php</p><p>&nbsp; &nbsp; &nbsp;```</p><p><br/></p><p>&nbsp; &nbsp; &nbsp;根据参数 count 的值，移除列表中与参数 value 相等的元素。`count`的值可以是以下几种：</p><p><br/></p><p>&nbsp; &nbsp; &nbsp;- count &gt; 0：从表头开始向表尾搜索，移除与`value`相等的元素，数量为`count`。</p><p>&nbsp; &nbsp; &nbsp;- count &lt; 0：从表尾开始向表头搜索，移除与 `value`相等的元素，数量为`count`的绝对值。</p><p>&nbsp; &nbsp; &nbsp;- count = 0：移除表中所有与`value` 相等的值。</p><p><br/></p><p>10. `set`集合的操作：</p><p><br/></p><p>&nbsp; &nbsp; - 添加元素：</p><p><br/></p><p>&nbsp; &nbsp; &nbsp; ```</p><p>&nbsp; &nbsp; &nbsp; &nbsp; sadd set value1 value2....</p><p>&nbsp; &nbsp; &nbsp; &nbsp; 如：</p><p>&nbsp; &nbsp; &nbsp; &nbsp; sadd team xiaotuo datuo</p><p>&nbsp; &nbsp; &nbsp; ```</p><p><br/></p><p>&nbsp; &nbsp; - 查看元素：</p><p><br/></p><p>&nbsp; &nbsp; &nbsp; ```</p><p>&nbsp; &nbsp; &nbsp; &nbsp; smembers set</p><p>&nbsp; &nbsp; &nbsp; &nbsp; 如：</p><p>&nbsp; &nbsp; &nbsp; &nbsp; smembers team</p><p>&nbsp; &nbsp; &nbsp; ```</p><p><br/></p><p>&nbsp; &nbsp; - 移除元素：</p><p><br/></p><p>&nbsp; &nbsp; &nbsp; ```</p><p>&nbsp; &nbsp; &nbsp; &nbsp; srem set member...</p><p>&nbsp; &nbsp; &nbsp; &nbsp; 如：</p><p>&nbsp; &nbsp; &nbsp; &nbsp; srem team xiaotuo datuo</p><p>&nbsp; &nbsp; &nbsp; ```</p><p><br/></p><p>&nbsp; &nbsp; - 查看集合中的元素个数：</p><p><br/></p><p>&nbsp; &nbsp; &nbsp; ```</p><p>&nbsp; &nbsp; &nbsp; &nbsp; scard set</p><p>&nbsp; &nbsp; &nbsp; &nbsp; 如：</p><p>&nbsp; &nbsp; &nbsp; &nbsp; scard team1</p><p>&nbsp; &nbsp; &nbsp; ```</p><p><br/></p><p>&nbsp; &nbsp; - 获取多个集合的交集：</p><p><br/></p><p>&nbsp; &nbsp; &nbsp; ```</p><p>&nbsp; &nbsp; &nbsp; &nbsp; sinter set1 set2</p><p>&nbsp; &nbsp; &nbsp; &nbsp; 如：</p><p>&nbsp; &nbsp; &nbsp; &nbsp; sinter team1 team2</p><p>&nbsp; &nbsp; &nbsp; ```</p><p><br/></p><p>&nbsp; &nbsp; - 获取多个集合的并集：</p><p><br/></p><p>&nbsp; &nbsp; &nbsp; ```</p><p>&nbsp; &nbsp; &nbsp; &nbsp; sunion set1 set2</p><p>&nbsp; &nbsp; &nbsp; &nbsp; 如：</p><p>&nbsp; &nbsp; &nbsp; &nbsp; sunion team1 team2</p><p>&nbsp; &nbsp; &nbsp; ```</p><p><br/></p><p>&nbsp; &nbsp; - 获取多个集合的差集：</p><p><br/></p><p>&nbsp; &nbsp; &nbsp; ```</p><p>&nbsp; &nbsp; &nbsp; sdiff set1 set2</p><p>&nbsp; &nbsp; &nbsp; 如：</p><p>&nbsp; &nbsp; &nbsp; sdiff team1 team2</p><p>&nbsp; &nbsp; &nbsp; ```</p><p><br/></p><p>11. `hash`哈希操作：</p><p><br/></p><p>&nbsp; &nbsp; - 添加一个新值：</p><p><br/></p><p>&nbsp; &nbsp; &nbsp; ```</p><p>&nbsp; &nbsp; &nbsp; &nbsp; hset key field value</p><p>&nbsp; &nbsp; &nbsp; &nbsp; 如：</p><p>&nbsp; &nbsp; &nbsp; &nbsp; hset website baidu baidu.com</p><p>&nbsp; &nbsp; &nbsp; ```</p><p><br/></p><p>&nbsp; &nbsp; &nbsp; 将哈希表`key`中的域`field`的值设为`value`。</p><p>&nbsp; &nbsp; &nbsp; 如果`key`不存在，一个新的哈希表被创建并进行 `HSET`操作。如果域 `field`已经存在于哈希表中，旧值将被覆盖。</p><p><br/></p><p>&nbsp; &nbsp; - 获取哈希中的`field`对应的值：</p><p><br/></p><p>&nbsp; &nbsp; &nbsp; ```</p><p>&nbsp; &nbsp; &nbsp; &nbsp; hget key field</p><p>&nbsp; &nbsp; &nbsp; &nbsp; 如：</p><p>&nbsp; &nbsp; &nbsp; &nbsp; hget website baidu</p><p>&nbsp; &nbsp; &nbsp; ```</p><p><br/></p><p>&nbsp; &nbsp; - 删除`field`中的某个`field`：</p><p><br/></p><p>&nbsp; &nbsp; &nbsp; ```</p><p>&nbsp; &nbsp; &nbsp; &nbsp; hdel key field</p><p>&nbsp; &nbsp; &nbsp; &nbsp; 如：</p><p>&nbsp; &nbsp; &nbsp; &nbsp; hdel website baidu</p><p>&nbsp; &nbsp; &nbsp; ```</p><p><br/></p><p>&nbsp; &nbsp; - 获取某个哈希中所有的`field`和`value`：</p><p><br/></p><p>&nbsp; &nbsp; &nbsp; ```</p><p>&nbsp; &nbsp; &nbsp; &nbsp; hgetall key</p><p>&nbsp; &nbsp; &nbsp; &nbsp; 如：</p><p>&nbsp; &nbsp; &nbsp; &nbsp; hgetall website</p><p>&nbsp; &nbsp; &nbsp; ```</p><p><br/></p><p>&nbsp; &nbsp; - 获取某个哈希中所有的`field`：</p><p><br/></p><p>&nbsp; &nbsp; &nbsp; ```</p><p>&nbsp; &nbsp; &nbsp; &nbsp; hkeys key</p><p>&nbsp; &nbsp; &nbsp; &nbsp; 如：</p><p>&nbsp; &nbsp; &nbsp; &nbsp; hkeys website</p><p>&nbsp; &nbsp; &nbsp; ```</p><p><br/></p><p>&nbsp; &nbsp; - 获取某个哈希中所有的值：</p><p><br/></p><p>&nbsp; &nbsp; &nbsp; ```</p><p>&nbsp; &nbsp; &nbsp; hvals key</p><p>&nbsp; &nbsp; &nbsp; 如：</p><p>&nbsp; &nbsp; &nbsp; hvals website</p><p>&nbsp; &nbsp; &nbsp; ```</p><p><br/></p><p>&nbsp; &nbsp; - 判断哈希中是否存在某个`field`：</p><p><br/></p><p>&nbsp; &nbsp; &nbsp; ```</p><p>&nbsp; &nbsp; &nbsp; hexists key field</p><p>&nbsp; &nbsp; &nbsp; 如：</p><p>&nbsp; &nbsp; &nbsp; hexists website baidu</p><p>&nbsp; &nbsp; &nbsp; ```</p><p><br/></p><p>&nbsp; &nbsp; - 获取哈希中总共的键值对：</p><p><br/></p><p>&nbsp; &nbsp; &nbsp; ```</p><p>&nbsp; &nbsp; &nbsp; hlen field</p><p>&nbsp; &nbsp; &nbsp; 如：</p><p>&nbsp; &nbsp; &nbsp; hlen website</p><p>&nbsp; &nbsp; &nbsp; ```</p><p><br/></p><p>12. 事务操作：Redis事务可以一次执行多个命令，事务具有以下特征：</p><p><br/></p><p>&nbsp; &nbsp; - 隔离操作：事务中的所有命令都会序列化、按顺序地执行，不会被其他命令打扰。</p><p><br/></p><p>&nbsp; &nbsp; - 原子操作：事务中的命令要么全部被执行，要么全部都不执行。</p><p><br/></p><p>&nbsp; &nbsp; - 开启一个事务：</p><p><br/></p><p>&nbsp; &nbsp; &nbsp; ```</p><p>&nbsp; &nbsp; &nbsp; &nbsp; multi</p><p>&nbsp; &nbsp; &nbsp; ```</p><p><br/></p><p>&nbsp; &nbsp; &nbsp; 以后执行的所有命令，都在这个事务中执行的。</p><p><br/></p><p>&nbsp; &nbsp; - 执行事务：</p><p><br/></p><p>&nbsp; &nbsp; &nbsp; ```</p><p>&nbsp; &nbsp; &nbsp; &nbsp; exec</p><p>&nbsp; &nbsp; &nbsp; ```</p><p><br/></p><p>&nbsp; &nbsp; &nbsp; 会将在`multi`和`exec`中的操作一并提交。</p><p><br/></p><p>&nbsp; &nbsp; - 取消事务：</p><p><br/></p><p>&nbsp; &nbsp; &nbsp; ```</p><p>&nbsp; &nbsp; &nbsp; &nbsp; discard</p><p>&nbsp; &nbsp; &nbsp; ```</p><p><br/></p><p>&nbsp; &nbsp; &nbsp; 会将`multi`后的所有命令取消。</p><p><br/></p><p>&nbsp; &nbsp; - 监视一个或者多个`key`：</p><p><br/></p><p>&nbsp; &nbsp; &nbsp; ```</p><p>&nbsp; &nbsp; &nbsp; &nbsp; watch key...</p><p>&nbsp; &nbsp; &nbsp; ```</p><p><br/></p><p>&nbsp; &nbsp; &nbsp; 监视一个(或多个)key，如果在事务执行之前这个(或这些) key被其他命令所改动，那么事务将被打断。</p><p><br/></p><p>&nbsp; &nbsp; - 取消所有`key`的监视：</p><p><br/></p><p>&nbsp; &nbsp; &nbsp; ```</p><p>&nbsp; &nbsp; &nbsp; &nbsp; unwatch</p><p>&nbsp; &nbsp; &nbsp; ```</p><p><br/></p><p>13. 发布/订阅操作：</p><p><br/></p><p>&nbsp; &nbsp; - 给某个频道发布消息：</p><p><br/></p><p>&nbsp; &nbsp; &nbsp; ```</p><p>&nbsp; &nbsp; &nbsp; &nbsp; publish channel message</p><p>&nbsp; &nbsp; &nbsp; ```</p><p><br/></p><p>&nbsp; &nbsp; - 订阅某个频道的消息：</p><p><br/></p><p>&nbsp; &nbsp; &nbsp; ```</p><p>&nbsp; &nbsp; &nbsp; &nbsp; subscribe channel</p><p>&nbsp; &nbsp; &nbsp; ```</p><p><br/></p>','2019-04-20'),(10,'Scrapy-Redis分布式爬虫组件','<p># Scrapy-Redis分布式爬虫组件</p><p><br/></p><p>`Scrapy`是一个框架，他本身是不支持分布式的。如果我们想要做分布式的爬虫，就需要借助一个组件叫做`Scrapy-Redis`，这个组件正是利用了`Redis`可以分布式的功能，集成到`Scrapy`框架中，使得爬虫可以进行分布式。可以充分的利用资源（多个ip、更多带宽、同步爬取）来提高爬虫的爬行效率。</p><p><br/></p><p>## 分布式爬虫的优点：</p><p><br/></p><p>1. 可以充分利用多台机器的带宽。</p><p>2. 可以充分利用多台机器的ip地址。</p><p>3. 多台机器做，爬取效率更高。</p><p><br/></p><p>## 分布式爬虫必须要解决的问题：</p><p><br/></p><p>1. 分布式爬虫是好几台机器在同时运行，如何保证不同的机器爬取页面的时候不会出现重复爬取的问题。</p><p>2. 同样，分布式爬虫在不同的机器上运行，在把数据爬完后如何保证保存在同一个地方。</p><p><br/></p><p>## 安装：</p><p><br/></p><p>通过`pip install scrapy-redis`即可安装。</p><p><br/></p><p>## Scrapy-Redis架构：</p><p><br/></p><p>Scrapy架构图：</p><p>![img](scrapy_all.png)</p><p><br/></p><p>Scrapy-Redis架构图：</p><p>![img](scrapy-redis.png)</p><p><br/></p><p>分布式爬虫架构图：</p><p>![img](fenbushi.png)</p><p><br/></p><p>以上两个图片对比我们可以发现。`Item Pipeline`在接收到数据后发送给了`Redis`、`Scheduler`调度器调度数据也是从`Redis`中来的、并且其实数据去重也是在`Redis`中做的。</p><p><br/></p><p>## 编写Scrapy-Redis分布式爬虫：</p><p><br/></p><p>要将一个`Scrapy`项目变成一个`Scrapy-redis`项目只需修改以下三点就可以了：</p><p><br/></p><p>1. 将爬虫的类从`scrapy.Spider`变成`scrapy_redis.spiders.RedisSpider`；或者是从`scrapy.CrawlSpider`变成`scrapy_redis.spiders.RedisCrawlSpider`。</p><p>2. 将爬虫中的`start_urls`删掉。增加一个`redis_key=&quot;xxx&quot;`。这个`redis_key`是为了以后在`redis`中控制爬虫启动的。爬虫的第一个url，就是在redis中通过这个发送出去的。</p><p>3. 在配置文件中增加如下配置：</p><p><br/></p><p>```</p><p>&nbsp; &nbsp; # Scrapy-Redis相关配置</p><p>&nbsp; &nbsp; # 确保request存储到redis中</p><p>&nbsp; &nbsp; SCHEDULER = &quot;scrapy_redis.scheduler.Scheduler&quot;</p><p><br/></p><p>&nbsp; &nbsp; # 确保所有爬虫共享相同的去重指纹</p><p>&nbsp; &nbsp; DUPEFILTER_CLASS = &quot;scrapy_redis.dupefilter.RFPDupeFilter&quot;</p><p><br/></p><p>&nbsp; &nbsp; # 设置redis为item pipeline</p><p>&nbsp; &nbsp; ITEM_PIPELINES = {</p><p>&nbsp; &nbsp; &nbsp; &nbsp; &#39;scrapy_redis.pipelines.RedisPipeline&#39;: 300</p><p>&nbsp; &nbsp; }</p><p><br/></p><p>&nbsp; &nbsp; # 在redis中保持scrapy-redis用到的队列，不会清理redis中的队列，从而可以实现暂停和恢复的功能。</p><p>&nbsp; &nbsp; SCHEDULER_PERSIST = True</p><p><br/></p><p>&nbsp; &nbsp; # 设置连接redis信息</p><p>&nbsp; &nbsp; REDIS_HOST = &#39;127.0.0.1&#39;</p><p>&nbsp; &nbsp; REDIS_PORT = 6379</p><p>```</p><p><br/></p><p>1. 运行爬虫：</p><p>&nbsp; &nbsp;1. 在爬虫服务器上。进入爬虫文件所在的路径，然后输入命令：`scrapy runspider [爬虫名字]`。</p><p><br/></p><p>&nbsp; &nbsp; &nbsp; ```</p><p>&nbsp; &nbsp; &nbsp; 进入项目spiders所在的目录</p><p>&nbsp; &nbsp; &nbsp; ```</p><p><br/></p><p>&nbsp; &nbsp; &nbsp;&nbsp;</p><p><br/></p><p>&nbsp; &nbsp;2. 在`Redis`服务器上，推入一个开始的url链接：`redis-cli&gt; lpush [redis_key] start_url`开始爬取。</p><p><br/></p>','2019-04-20'),(11,'requirement','<p>asn1crypto==0.24.0</p><p>attrs==19.1.0</p><p>Automat==0.7.0</p><p>beautifulsoup4==4.7.1</p><p>certifi==2019.3.9</p><p>cffi==1.12.2</p><p>chardet==3.0.4</p><p>constantly==15.1.0</p><p>cryptography==2.6.1</p><p>cssselect==1.0.3</p><p>dukpy==0.2.2</p><p>future==0.17.1</p><p>gevent==1.4.0</p><p>greenlet==0.4.15</p><p>html5lib==1.0.1</p><p>hyperlink==19.0.0</p><p>idna==2.8</p><p>incremental==17.5.0</p><p>javascripthon==0.10</p><p>Jinja2==2.10</p><p>jupyter-echarts-pypkg==0.1.2</p><p>lml==0.0.2</p><p>lxml==4.3.3</p><p>macropy3==1.1.0b2</p><p>MarkupSafe==1.1.1</p><p>parsel==1.5.1</p><p>Pillow==6.0.0</p><p>pyasn1==0.4.5</p><p>pyasn1-modules==0.2.4</p><p>pycharts==0.1.5</p><p>pycparser==2.19</p><p>PyDispatcher==2.0.5</p><p>pyecharts==0.5.11</p><p>pyecharts-javascripthon==0.0.6</p><p>pyecharts-jupyter-installer==0.0.3</p><p>pyecharts-snapshot==0.1.10</p><p>PyHamcrest==1.9.0</p><p>PyMySQL==0.9.3</p><p>pyOpenSSL==19.0.0</p><p>pypiwin32==223</p><p>pytesseract==0.2.6</p><p>pywin32==224</p><p>queuelib==1.5.0</p><p>redis==3.2.1</p><p>requests==2.21.0</p><p>Scrapy==1.6.0</p><p>scrapy-redis==0.6.8</p><p>selenium==3.141.0</p><p>service-identity==18.1.0</p><p>six==1.12.0</p><p>soupsieve==1.9</p><p>Twisted==18.9.0</p><p>urllib3==1.24.1</p><p>w3lib==1.20.0</p><p>webencodings==0.5.1</p><p>zope.interface==4.6.0</p><p><br/></p>','2019-04-20');
/*!40000 ALTER TABLE `article` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `backuser`
--

DROP TABLE IF EXISTS `backuser`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `backuser` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `username` varchar(100) DEFAULT NULL,
  `password` varchar(256) DEFAULT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `username` (`username`)
) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `backuser`
--

LOCK TABLES `backuser` WRITE;
/*!40000 ALTER TABLE `backuser` DISABLE KEYS */;
INSERT INTO `backuser` VALUES (1,'zmr','441621');
/*!40000 ALTER TABLE `backuser` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `photo`
--

DROP TABLE IF EXISTS `photo`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `photo` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `url` varchar(100) DEFAULT NULL,
  `title` varchar(100) DEFAULT NULL,
  `context` varchar(255) DEFAULT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `url` (`url`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `photo`
--

LOCK TABLES `photo` WRITE;
/*!40000 ALTER TABLE `photo` DISABLE KEYS */;
/*!40000 ALTER TABLE `photo` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `user`
--

DROP TABLE IF EXISTS `user`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `user` (
  `id` int(11) NOT NULL AUTO_INCREMENT,
  `username` varchar(100) DEFAULT NULL,
  `password` varchar(256) DEFAULT NULL,
  `article_id` int(11) DEFAULT NULL,
  PRIMARY KEY (`id`),
  UNIQUE KEY `username` (`username`),
  KEY `article_id` (`article_id`),
  CONSTRAINT `user_ibfk_1` FOREIGN KEY (`article_id`) REFERENCES `article` (`id`)
) ENGINE=InnoDB AUTO_INCREMENT=33 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `user`
--

LOCK TABLES `user` WRITE;
/*!40000 ALTER TABLE `user` DISABLE KEYS */;
INSERT INTO `user` VALUES (21,'3sdf4165sd','sdfsdfs',NULL),(22,'dfhgdf','dfgdf',NULL),(24,'123','123',NULL),(26,'23165','',NULL),(27,'5644','153156',NULL),(28,'441621','441621',NULL),(29,'333','333',NULL),(31,'','',NULL),(32,'888','888',NULL);
/*!40000 ALTER TABLE `user` ENABLE KEYS */;
UNLOCK TABLES;
/*!40103 SET TIME_ZONE=@OLD_TIME_ZONE */;

/*!40101 SET SQL_MODE=@OLD_SQL_MODE */;
/*!40014 SET FOREIGN_KEY_CHECKS=@OLD_FOREIGN_KEY_CHECKS */;
/*!40014 SET UNIQUE_CHECKS=@OLD_UNIQUE_CHECKS */;
/*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */;
/*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */;
/*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */;
/*!40111 SET SQL_NOTES=@OLD_SQL_NOTES */;

-- Dump completed on 2019-04-20 14:51:06
